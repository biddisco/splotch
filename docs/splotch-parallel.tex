% Template for Elsevier CRC journal article
% version 1.0 dated 13 October 2009

% This file (c) 2009 Elsevier Ltd.  Modifications may be freely made,
% provided the edited file is saved under a different name

% This file contains modifications for Procedia Computer Science

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% This template uses the elsarticle.cls document class and the extension package ecrc.sty
%% For full documentation on usage of elsarticle.cls, consult the documentation "elsdoc.pdf"
%% Further resources available at http://www.elsevier.com/latex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% The '1p' and 'times' class options of elsarticle are used for Elsevier CRC
\documentclass[1p,times]{elsarticle}

%% The `ecrc' package must be called to make the CRC functionality available
\usepackage{ecrc}

%% The ecrc package defines commands needed for running heads and logos.
%% For running heads, you can set the journal name, the volume, the starting page and the authors

%% set the volume if you know. Otherwise `00'
\volume{00}

%% set the starting page if not 1
\firstpage{1}

%% Give the name of the journal
\journalname{Procedia Computer Science}

%% Give the author list to appear in the running head
%% Example \runauth{C.V. Radhakrishnan et al.}
\runauth{Z. Jin et al.} 

%% The choice of journal logo is determined by the \jid and \jnltitlelogo commands.
%% A user-supplied logo with the name <\jid>logo.pdf will be inserted if present.
%% e.g.\ if \jid{yspmi} the system will look for a file yspmilogo.pdf
%% Otherwise the content of \jnltitlelogo will be set between horizontal lines as a default logo

%% Give the abbreviation of the Journal.
\jid{procs}

%% Give a short journal name for the dummy logo (if needed)
\jnltitlelogo{Procedia Computer Science}

%% Hereafter the template follows `elsarticle'.
%% For more details see the existing template files elsarticle-template-harv.tex and elsarticle-template-num.tex.

%% Elsevier CRC generally uses a numbered reference style
%% For this, the conventions of elsarticle-template-num.tex should be followed (included below)
%% If using BibTeX, use the style file elsarticle-num.bst

%% End of ecrc-specific commands
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
%% \usepackage{lineno}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}

% if you have landscape tables
%%%\usepackage[figuresright]{rotating}

% put your own definitions here:
%   \newcommand{\cZ}{\cal{Z}}
%   \newtheorem{def}{Definition}[section]
%   ...

% add words to TeX's hyphenation exception list
%\hyphenation{author another created financial paper re-commend-ed Post-Script}

% declarations for front matter

\newcommand{\aj}{AJ}
\newcommand{\apj}{ApJ}
\newcommand{\apjl}{ApJ}
\newcommand{\apjs}{ApJS}
\newcommand{\aap}{A\&A}
\newcommand{\aaps}{A\&AS}
\newcommand{\mnras}{MNRAS}
\newcommand{\nat}{Nature}
\newcommand{\araa}{ARAA}
\newcommand{\prd}{Phys. Rev. D}
\newcommand{\pasj}{PASJ}
\newcommand{\pasp}{PASP}
\newcommand{\physrep}{Phys. Rep.}


\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\dochead{}
%% Use \dochead if there is an article header, e.g.\ \dochead{Short communication}

\title{High-Performance Astrophysical Visualization Using Splotch}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

\author{Zhefan Jin, Mel Krokos}
\ead{Zhefan.Jin@port.ac.uk, mel.krokos@port.ac.uk}
\address{School of Creative Technologies, University of Portsmouth, Winston Churchill Avenue, Portsmouth, United Kingdom}

\author{Marzia Rivi, Claudio Gheller}
\ead{m.rivi@cineca.it, c.gheller@cineca.it}
\address{CINECA, Via Magnanelli 6/3, Casalecchio di Reno, Italy}

\author{Klaus Dolag, Martin Reinecke}
\ead{kdolag@mpa-garching.mpg.de, martin@mpa-garching.mpg.de}
\address{Max-Planck-Institut f\"ur Astrophysik, Karl-Schwarzschild Strasse 1, Garching bei M\"unchen, Germany}

%\author{Martin Reinecke}
%\ead{martin@mpa-garching.mpg.de}
%\address{Max-Planck-Institut f\"ur Astrophysik, Karl-Schwarzschild Strasse 1, Garching bei M\"unchen, Germany}
%
%\author{Claudio Gheller}
%\ead{c.gheller@cineca.it}
%\address{CINECA, Via Magnanelli 6/3, Casalecchio di Reno, Italy}
%
%\author{Mel Krokos}
%\ead{mel.krokos@port.ac.uk}
%\address{School of Creative Technologies, University of Portsmouth, Winston Churchill Avenue, Portsmouth, United Kingdom}

\begin{abstract}

Current technological advances impact profoundly on the unprecedented growth 
in the quality and quantity of astrophysical data sets. The main characteristic
is extremely large sizes (many GB) while forthcoming next-generation astrophysical
data sets are expected to exhibit massively large sizes (many TB). Visual data
exploration and discovery tools are robust instruments for rapidly and intuitively 
inspecting very large-scale data sets to identify regions of interest within
which to apply time-consuming algorithms.  This paper reports on recent developments 
for a high performance implementation of {\tt Splotch}, our previously developed 
ray-tracing algorithm for effective visualization of large-scale astrophysical 
data sets coming from particle-based simulations. We discuss several approaches for
parallelizing Splotch to suit multicore CPUs and CUDA-enabled GPUs. 
We present benchmarks of our implementations, with a special focus on the 
Millennium II simulation. Finally we summarise the work concluding with pointers 
to future developments.

\end{abstract}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword


Visual Discovery \sep Splotch \sep Large-Scale Particle-Based Numerical Simulations\sep High-Performance Visualization \sep OpenMP \sep CUDA-enabled GPUs\sep Millenium II Simulation 


%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%%
%% Start line numbering here if you want
%%
% \linenumbers

\section{Introduction}
\label{intro}

Nowadays the technological advances in instrumentation and computing 
capability impact profoundly on the dramatic growth in the quality and 
quantity of astrophysical data sets obtained from observational instruments,
e.g.\ sky surveys \cite{sdss}, \cite{lofar}, or large-scale numerical
simulations, e.g.\ the Millennium II simulation \cite{2009MNRAS.398.1150B}.

The main characteristic of modern astrophysical data sets is extremely large
sizes (on the order of hundreds of GB) requiring storage in extremely
large-scale distributed databases. The forthcoming next-generation astrophysical 
data sets are expected to exhibit massively large sizes (in the order of hundreds
of TB), e.g.\ \cite{lsst}.
To obtain a comprehensive insight into modern astrophysical data sets, astronomers
employ sophisticated data mining algorithms, often at high computational costs. 
Visual data exploration and discovery tools are then exploited in order to rapidly 
and intuitively inspect very large-scale data sets to identify regions of interest
within which to apply time-consuming algorithms. Such tools are based on a 
combination of meaningful data {\it visualizations} and user interaction with them.  

%An effective visualizations based apporoach
%can be a very intuitive and ready way of discovering and 
%understanding rapidly brand new correlations, similarities and data patterns. 
%For on-going processes, e.g.\ a numerical simulation in progress, advanced 
%visual data exploration and discovery allow constant monitoring and - if 
%anomalies are discovered - prompt correction of the run, thus saving valuable 
%time and resources. For such reasons the astronomical community has always 
%dedicated special attention to advanced graphical/visualization tools driving 
%their evolution or even being directly involved in developing them.

The data exploration tools traditionally employed by astronomers 
are limited either 
to processing and displaying of 2D images (see, e.g., \cite{iraf},
\cite{midas}, \cite{sao},
\cite{gaia}) or to
generation of meaningful 2D and 3D plots (e.g.\ \cite{gnuplot},
\cite{supermongo}, \cite{idl}).
 
%(e.g.\ IRAF \cite{iraf}, by NOAO; 
%ESO-MIDAS \cite{midas} by the European Southern Observatory; SaoImage \cite{sao} 
%by the Smithsonian Astrophysical Observatory and GAIA \cite{gaia} by ESO) or on 
%generation of meaningful 2D and 3D plots (e.g.\ Gnuplot \cite{gnuplot}; 
%SuperMongo \cite{supermongo} and IDL \cite{idl} by ITT Visual Information). 
%There is a plethora of other tools, for details the reader is referred to 
%dedicated literature reviews.
%, e.g.\ \cite{survey}.

To overcome the shortcomings of traditional tools, a new generation of software 
packages is now emerging, providing astronomers with robust instruments in the context 
of large-scale astrophysical data sets (e.g.\ \cite{paraview}, \cite{aladin},
\cite{topcat}, \cite{visivo1} and \cite{visivo2}, 
\cite{3dslicer}, \cite{splash} and \cite{visit}). 
The underlying principles are exploitation of high performance architectures 
(i.e.\ multicore CPUs and powerful graphics boards), interoperability
(different applications can operate simultaneously on
shared data sets) and collaborative workflows (permitting several users to
work simultaneously for exchanging information and visualization experiences).

%ParaView \cite{paraview} was developed specifically for handling large-scale 
%data sets efficiently by exploiting distributed memories. Aladin \cite{aladin} is
%an interactive sky atlas for visualization of astronomical images supporting 
%access to astronomical databases (such as Simbad \cite{simbad} and VizieR \cite{vizier}). 
%TopCat \cite{topcat} is an application that includes several tools for analysis 
%and visualization of astronomical tables. VisIVO \cite{visivo1} is an integrated 
%suite of tools and services specifically designed for multidimensional visualisation. 
%VisIVO Server is a recent development for visualization of large-scale astrophysical 
%data sets \cite{visivo2}.
%
%Other examples of new-generation software packages for astrophysical visualization 
%are 3D slicer \cite{3dslicer} capable of displaying volumetric data sets and user
%defined cut throughs, SPLASH \cite{splash} specifically designed in the context 
%of astrophysical simulations and VisIt \cite{visit} which is a parallel visualization
%and data analysis application supporting a rich set of visualisation features, 
%e.g.\ displaying of scalar and vector fields defined on 2D or 3D structured 
%and unstructured meshes. 

This paper describes a high performance implementation of 
{\tt Splotch} \citep{2008NJPh...10l5006D}, our previously developed ray-tracing 
algorithm for effective visualization of large-scale astrophysical data sets coming 
from particle-based computer simulations. N-Body simulations constitute 
prime examples of particle-based simulations, typically associated with very 
large-scale data sets, e.g.\ the Millennium II simulation \citep{2009MNRAS.398.1150B}.
This is a simulation of the evolution of a meaningful fraction of the universe 
by means of 10 billion fluid elements ({\it particles}) interacting with each other 
through gravitational forces. The typical size of a snapshot of the Millennium II 
simulation is about 400 GB representing a particle's ID, position and velocity
together with additional properties, e.g.\ local smoothing length, density and 
velocity dispersion. For further details on the Millennium II simulation and
other works about the visualization of its data sets, the reader
is referred to \citep{2009MNRAS.398.1150B}, \cite{fraedrich2009} and \cite{Szalay2008}.

%The fundamentals and the traditional sequential operation of {\tt Splotch} 
%\cite{2008NJPh...10l5006D} are reviewed in section 2. Section 3 discusses 
%our strategy for parallelizing Splotch based on different approaches that 
%are suitable for a variety of underlying architecture configurations. 
%Our implementations are Single Instruction Multiple Data (SIMD) designs 
%founded on the MPI library \cite{mpi} in order to support distributed multicore 
%CPUs, the OpenMP \cite{openmp} for supporting shared memory nodes and finally 
%CUDA \cite{cuda} for exploiting not only currently available but also forthcoming 
%next-generation multiple GPUs. The advantage of adopting several parallelization 
%solutions is that we can deploy them simultaneously on hybrid architectures, 
%e.g.\ mixed hardware architectures consisting of a large number of multicore CPUs 
%and CUDA-enabled GPUs. Benchmarks for our parallelization designs and a discussion 
%on the Millenium II visualization are presented in section 4. Finally section 5 
%outlines a summary of our work and includes pointers to future developments.


\section{The Splotch Algorithm}
\label{splotch}

\begin{figure}
\begin{center}
\includegraphics[width=0.40\textwidth]{millenium2_veldisp.pdf}
\includegraphics[width=0.40\textwidth]{millenium2_vel.pdf}
\end{center}
\caption{A visualization  of the MillenniumII simulation \citep{2009MNRAS.398.1150B}. The color transfer function uses a particle's velocity dispersion (left) and 3D velocity (right).}\label{mil2}
\end{figure}

The rendering algorithm of {\tt Splotch} is generally designed to handle
point-like particle distributions. Such tracer particles can be smoothed
to obtain a continuous field, which is rendered
%, most commonly the $B_2$-Spline \cite{1985A&A...149..135M}
%\begin{equation}
%   W(x,h)=\frac{8}{\pi h^3}\left\{\begin{array}{ll}
%      1 - 6 \left(\frac{x}{h}\right)^2 + 6 \left(\frac{x}{h}\right)^3 \;\;& 0 \le \frac{x}{h} < 0.5 \\
%      2 \left(1 - \frac{x}{h}\right)^3                              & 0.5 \le \frac{x}{h} < 1 \\
%      0                                                             & 1 \le \frac{x}{h} \\
%   \end{array} \right. , \label{SPH:kern}
%\end{equation}
%where $h$ is the local smoothing length, which is typically defined in a way
%that every particle overlaps with $\approx 32$ neighbors. Therefore, the rendering is based
on the following assumptions:

\begin{itemize}
\item
The contribution to the matter density by every particle can
be described as a Gaussian distribution 
$\rho_p(\vec r)=\rho_{0,p}\exp(-r^2/\sigma_p^2)$.
%\footnote{Note that the
%$b_2$-Spline kernel used in SPH has a shape very similar to the Gaussian distribution.}
In practice, it is much more handy to have a compact support of the
distribution, and therefore the distribution is set to zero at a given
distance $f\cdot\sigma_p$, where $f$ is a proper multiplicative factor. 
%Following the approach often used to vizualize 
%cosmological simulation we choose $f$ in such a way that
%$f\cdot\sigma_p$ is related to the smoothing length $h$, i.e.
%to fulfill $h \approx f\cdot\sigma_p$. 
Therefore rays passing
the particle at a distance larger than $f\cdot\sigma_p$ will be
unaffected by the particle's density distribution.

\item 
We use three ``frequencies'' to describe the red, green and blue
components of the radiation, respectively. These are treated independently.

\item
The radiation intensity $\bf{I}$\footnote{Here we treat all
intensities as vectors with r,g and b components.} along a ray through the simulation
volume is modeled by the well known radiative transfer equation
\begin{equation}
\frac{d\bf{I}(x)}{dx}=(\bf{E}_p-\bf{A}_p\bf{I}(x))\rho_p(x),
\end{equation}
which can be found in standard textbooks \cite{1991par..book.....S}.
Here, $\bf{E}_p$ and $\bf{A}_p$ describe the strength of radiation emission and absorption
for a given particle for the three rgb-colour components. In general it is recommended to
set $\bf{E}_p=\bf{A}_p$, which typically produces visually appealing images. This is presently a 
necessary setting for Splotch, in order to reduce the complexity of some aspects of its parallel
implementation. This constraint will be eliminated in the next releases of the code.
%; for special
%effects, however, independent emission and absorption coefficients can be used. Specially typically
%a small gray component (e.g.\ a small, constant addition to the rgb-components) can be added 
%to the absortpion of each particle ($\bf{A}_p$) to improve the three dimensional impression.
%In general the coefficients can vary between particles, and are typically chosen as a function 
%of a characteristic particle property. 
If a scalar quantity is chosen (e.g.\ the particle temperature,
density, velocity dispersion, etc.), the mapping to the three components of $\bf{E}$ and $\bf{A}$ (for red, green and blue)
is typically achieved via a transfer function, realized by a colour look-up table or palette, which can
be provided to the ray-tracer as an external file to allow a maximum of flexibility. If a
vector quantity is chosen (e.g.\ velocity, magnetic field, etc.), the three components of the vectors
can be mapped to the three components of $\bf{E}$ and $\bf{A}$ (for red, green and blue). In addition 
to the color, the optical depth of each particle can be also modulated proportionally to another
scalar property (e.g.\ density, etc.).
\end{itemize}


%Assuming that absorption and emission are homogeneously mixed it
%can be shown that, whenever a ray traverses a particle, its intensity
%change is given by
%\begin{equation}
%\label{i_change}
%\bf{I}_{\mbox{after}}=(\bf{I}_{\mbox{before}}-\bf{E}_p/\bf{A}_p)
%\exp(-\bf{A}_p\int_{-\infty}^\infty\rho_p(x)dx) + \bf{E}_p/\bf{A}_p
%\end{equation}
%The integral in this equation is given by
%$\rho_{0,p}\sigma_p\exp{(-d_0^2/\sigma_p^2)}\sqrt{\pi}$, where $d_0$
%is the minimum distance between the ray and the particle center.
%
%Under the assumption that the particles do not overlap, the intensity
%reaching the observer could simply be calculated by applying this formula to
%all particles intersecting with the ray, in the order of decreasing distance
%to the observer. In reality, of course, the particles do overlap, but since
%the relative intensity changes due to a single particle can be assumed to be
%very small, this approach can nevertheless be used in good approximation.

Further details on the Splotch rendering algorithm can be found in \citep{2008NJPh...10l5006D}. 
Figure \ref{mil2} shows a visualization example of a large simulation
containing 10 billion particles.

\section{Parallel Implementation}
\label{parallel}

The {\tt Splotch} algorithm operational scenario consists of a number of 
stages summarized as follows: a) read data from one or more files; b) process data (e.g.\ for normalization); c) render data and d) save the final image. All these steps can be parallelized using a
Single Instruction Multiple Data (SIMD) approach. This involves distributing the data 
in a balanced way between different computing elements (or processing units) and each computing element performing the same operations on its associated subset of data. Our parallelization has been achieved using different approaches suitable for a variety of underlying hardware architectures and software environments.

The MPI library \cite{mpi} has been used to define the overall data and work 
distribution. Data are read in chunks of the same (or similar) size by each processor
in the MPI pool. Then the same work is performed for steps 2 and 3 on each data chunk by the
corresponding processor. The final image is generated and saved only by the root processor. This is, in fact, a light task, which does not require any kind of parallel implementation.
The work accomplished in steps 2 and 3 can be further split, exploiting 
multicore shared memory processors or modern graphics boards, by means of
an OpenMP \cite{openmp} based approach or exploitation of the CUDA \cite{cuda} programming 
environment. 

These different parallel approaches can be used separately, 
if the available computing system fits only one of the available configurations, or jointly. 
For instance, on a single core PC with an Nvidia graphics card, only the CUDA based
parallelization strategy can be activated and exploited. On a multicore RVN \cite{rvn} 
node both MPI and CUDA can be used. This makes our parallel {\tt Splotch} code 
extremely flexible, portable, efficient and scalable. More details of our parallel implementations are presented in the rest of this section. Test results are discussed in section 4.

\subsection{MPI Implementation}
\label{mpi}

Once the data is distributed among the processors, all the remaining operations 
are performed locally and further communication is not needed, until the generation
of the final display. Each MPI process uses the assigned 
data to produce its own partial image. At the end, all the partial contributions are merged by means of a
collective reduction operation producing the final image. As the data load stage is the crucial step for balancing the overall workload and fast reading
data from the disk, we paid specific attention to efficient implementation of this functionality.
%In fact, from the results presented in section 4.1, it is clear how,
%as the data size grows,
%the data load process tends to be the most demanding section of the code. 

The adoption of MPI I/O based functions represents the ideal solution for obtaining
a high-performance, scalable data input utility.
With this approach each process has a different fileview of a single file 
that allows simultaneous and collective
writing/reading of noncontiguous interleaved data. 
%A view defines what data are visible 
%to each process and 
%consists of: an offset, measured in bytes from the beginning of the file; 
%an elementary type (etype) that 
%defines the unit of data access and positioning within the file; and a template for 
%accessing the file (filetype)
%consisting in a number of etypes and holes (which must be a multiple of the etype size). 
%The basic filetype is repeated again and again, tiling the file, and creating regions of allowed access 
%(where etypes are defined), and no access (where holes are defined).
Our implementation assumes that data are organized in the input file according to a block structure, 
where each block contains a single information for 
all $N$ particles. Therefore we have as many contiguous blocks
as the number $n$ of properties given for each particle, and we can see them as a 2-dimensional array $A$ 
of $n \times N$ float elements. 
Then, we have defined the MPI I/O filetype as a simple 2-dimensional subarray of $A$ 
of size $n \times N/nprocs$.

To support high performance computing environments where MPI I/O is not available, 
we have also provided two standard MPI binary readers based on standard {\tt fstream} functions.
%that can respectively 
%read binary data files written both in tab and block formats. 
%In both cases 
Data to be read are equally distributed among processes and simultaneously each one reads
their own portion of data by a direct access operation. 
%The block reader executes one single read of size $N/nprocs$ for each block of data, 
%while the tabular one reads $N/nprocs$ rows containing all $n$ quantities related to a single particle.
%
In all readers an endianness conversion is also performed if required.



\subsection{CUDA Implementation}
\label{cuda}
Nowadays Graphics Processing Units (GPUs) can offer a means of increased performance 
(often substantially) in the context of computationally intensive scientific applications by exploiting high speed underlying ALUs and stream data-parallel organization. 
The Compute Unified Device Architecture (CUDA) introduced by Nvidia offers access to highly parallellized modern GPU architectures via a 
simplified C language interface without demanding knowledge of the 
GPU operational details \cite{cuda}.
%This section presents our experiences 
%in developing a CUDA implementation for accelerating the Splotch %operation.
A thread on the GPU is extremely lightweight compared to CPU threads, so changing context among threads is not a costly operation. The minimum 
data chunks processed by a CUDA {\it multiprocessor} are numbers of threads ({\it warps}) handled in groupings as {\it blocks} and {\it grids} so that GPU-executed functions can exploit large numbers of threads. CUDA executes blocks sequentially
in case of limited hardware resources, but reverts to parallel execution for large numbers 
of processing units.  The resulting code can thus target simultaneously entry-level, high-end or even next-generation GPUs. Further details on CUDA can be found on \cite{cudaprogguide}.

%Although CUDA threads are scheduled for execution in terms of groups of warps, 
%threads in individual warps are executed in a Single Instruction Multiple Thread 
%(SIMT) way. Consequently assuming that a data-dependent conditional branch exists, 
%the execution of the relevant branch paths is effectively serialized. As an example 
%suppose that a warp contains 32 threads, the data-dependent conditional branch 
%involves paths A and B and 16 threads follow each path respectively. Consider the 
%situation in which the underlying multiprocessor executes A prior to B, then none of 
%the B threads will be executed until the A threads are completed. This situation 
%can cause a substantial slow-down in overall execution times. The worst-case 
%scenario may even involve only a single thread, the CUDA multiprocessor effectively 
%acting then as a single processor.

As soon as data is loaded in the memory of each processing unit\footnote{For our purposes a processing unit is defined as a multicore CPU together with a bundle of associated GPUs.}, our CUDA approach can be combined with the MPI parallelization strategy outlined in section 3.1. At that point in fact, processing units can be regarded as completely independent of each other, and CUDA can be exploited by determining each parallel task based on a single particle, that is a single particle is {\it processed} and {\it displayed} by a single CUDA thread. 
The processing involves normalizing of some particle values. 
The displaying involves transformation into screen coordinates, assigning of colours and rendering for determining screen areas affected by individual particles and subsequently combining them for final imaging.
%As there typically exists a significant number of data-dependent conditional branches 
%in the execution of Splotch, the MPI parallelization strategy outlined in section 3.1 
%is unsuitable for CUDA implementation. A further reason is that preserving the image 
%buffer storage requirements followed by the MPI implementation implies less memory 
%at our disposal (from the graphics card's on board memory) for running CUDA threads, 
%thus restricting full usage of the CUDA computational capability. 
%Based on these 
%considerations the overall amount of computation ({\it granularity}) involved in 

As modern particle-based simulations contain very large numbers of particles 
(on the order of millions) a large number of CUDA threads can be employed,
ensuring that underlying multiprocessors can be utilized to their full potential. 
The granularity during processing, transformation and colorization is more or 
less fixed among different particles. However during rendering it 
can vary considerably depending upon the number of screen pixels influenced 
by individual particles. As a worst case scenario consider two particles influencing 
all screen pixels and a single screen pixel respectively. Assuming they are handled by the same warp (this is determined by CUDA automatically), 
they are then scheduled to execute simultaneously. This unbalanced granularity 
can compromise significantly overall execution times (see Table 3).

%\begin{figure}
%\begin{center}
%\includegraphics[width=0.50\textwidth]{cu_splotch_process.png}
%\end{center}
%\caption{Overview of our CUDA implementation of Splotch. The GPU runs on the graphics board ({\it device}) while the CPU runs on the main computer ({\it host}). The red arrows represent execution flow of programs on the host and device (8 threads shown) respectively. An empty rectangle indicates a time-slot that a processing unit (device or host) is in idle state.}
%\end{figure}

To alleviate this situation we follow a 'split particles' strategy 
dividing large computational tasks into smaller average ones.  For any particle 
influencing a number of pixels that is larger than a threshold value, the relevant 
computational task is sub-divided into multiple ones, each associated with a subset 
of the original number of pixels. The threshold value 
can be determined in advance and given as input to {\tt Splotch}. To our experience a threshold close to 
the average of the width and height of the display window works satisfactorily. 
A shortcoming of this is the computational cost when doing the splitting; for an increased number of particles 
more threads are required. Execution of the splitting algorithm 
and memory copying among host and device involve additional costs. 
Nevertheless our results demonstrate improved timings (see Table 3).

%The overview of our CUDA implementation for accelerating the operation of Splotch is shown in Figure 2. The red arrows on the graphics board ({\it device}) represent CUDA threads running in parallel, for simplicity 8 threads are shown only. The aforementioned ranging is re-organized in three distinct phases so that global particle information, e.g.\ min/max values of some attributes, is calculated by the main computer ({\it host}) for improving the overall execution times. The host is also responsible for sorting the particles prior to colorization and moreover executing our 'split particles' algorithm. Finally during rendering the affected screen areas for individual particles are stored in a one-dimensional fragment buffer which is filled in through a render computation on the graphics board. The fragment buffer is then copied to the host which accumulatively combines all relevant fragment buffer computations for final imaging. As the device can be called asynchronously it can effectively operate in parallel with the host, so that render GPU computations and combine CPU operations can be performed simultaneously as shown below.

%\begin{verbatim}
%while  ( not all particles are rendered )
%{
%       find subset S(i) of particle array;
%A:     call device to render S(i);
%       if  ( S(i) is not first subset )
%       { 
%B:          combine with F(i-1), the output of S(i-1) in fragment buffer;
%       }
%C:     copy fragment buffer from device to host;
%       if  ( S(i) is the last subset )
%       {
%             combine with F(i);
%       }
%       i++;
%}
%\end{verbatim}

%The instruction A is the render computation executed on the graphics board in parallel with execution of instruction B, the combination operation carried out by the CPU. The results of A and B are merged by instruction C. Our test results indicate that overall times required for performing combination operations are mostly contained within the times demanded by render computations.

%\begin{table}
%\caption{The data sets used in the benchmarks for our parallel implementations of {\tt Splotch}}
%\begin{center}
%\begin{tabular}{|l|l|l|}
%\hline
%Label & 	Particles& 	Size (Bytes)  \\
%\hline
%1M   & 	1000000   & 24000000 \\
%\hline
%10M  & 	10000000  & 240000000 \\
%\hline
%100M & 	100000000 & 2400000000 \\
%\hline
%850M & 	882739509 & 21185748216 \\
%\hline
%M-II & 	10000000000 & 	280000000000 \\
%\hline
%\end{tabular}
%\end{center}
%\end{table}

\section{Benchmarks}

The parallelized {\tt Splotch} code has been tested on low-end and high-end hardware 
architectures using several test data sets in order to investigate applicability of
different parallelization approaches.

Exploiting the high portability of our code we could perform our tests on a 
5000 cores UNIX-AIX  SP6 system (referred to as SP6) and a Windows XP PC (referred to as Win). 
The SP6 system is a cluster of 168 Power6 575 computing nodes with 32 cores
and a memory of 128GB per node. The Win system is an Intel Xeon X5482 3.2 GHz CPU with 
two Nvidia Quadro FX 5600 graphics boards. Our target was to test parallelized
versions of Splotch on computing systems of different sizes and target applications, 
from a standard PC, where small to medium data sets can be used, up to high
performance platforms for handling very large data sets.

We used several benchmark data sets for our testing. The first
few data sets are derived from a cosmological N-Body simulation of more than 850 millions
particles characterized by spatial information together with velocities, mass density 
and smoothing length, for defining the size of the region influenced by the properties 
of each particle when deploying Splotch. We randomly extracted data sets containing
1, 10 and 100 million particles. These are indicated as 1M, 
10M and 100M tests. We also employed the data set 850M containing the entire simulation.
Such variety of data set sizes is required to match the memory available on
the different computing systems used for our testing.
Our most challenging data set comes from the Millennium II simulation \cite{2009MNRAS.398.1150B}
and consists of 10 billion particles. The data files employed are pure binaries organized in such a way that different
quantitites are stored consecutively (e.g.\ $x$ coordinates of all particles, then $y$ coordinates and so on) as expected by the MPI-IO based reader. 

\subsection{MPI Benchmarks}

\begin{figure}
\begin{center}
\includegraphics[width=0.49\textwidth]{bench100M_r.pdf}
\includegraphics[width=0.49\textwidth]{bench870M_r.pdf}
\end{center}
\caption{Scaling of the CPU time (total wallclock time) with the number of MPI threads 
used for vizualizing 100 million
and 850 million particles for an 800x800 pixel display.
Results for binary and MPI readers, in ST and SMT modes are shown}\label{mpi100M}
\end{figure}

Our test results on the SP6 platform for data sets 100M and 850M
are presented in Figure \ref{mpi100M}.
Power6 cores can schedule for execution two processes (or threads) in the same clock cycle, 
and it is also possible to use a single core as two virtual CPUs. This mode of Power6 
is called Simultaneous Multi-Threading (SMT), to distinguish it from 
the standard Single Thread mode (ST).
Deploying SMT notably improves the performance 
of processing and displaying in {\tt Splotch} by reducing the execution time up to  
$30\%$.

The 100M test (Figure \ref{mpi100M}, left panel), allows us 
to run on one processor fitting its memory. 
This is the estimate of the sequential performance of the code. 
The maximum number of processors for this test is set to 8, since the usage of 
more processors is inefficient, due to the small size 
of the data chunks assigned to each processor and the increasing overhead of communication.
On a single processor we get a best performance of about 123 seconds, which means that 
we can load and process approximatively 1 particle per $10^{-6}$ seconds. 
In all cases the code scales almost linearly up to 8 processors, beginning to lose
efficiency between 4 and 8 processors due to the reasons previously explained. 

The larger test, 850M (Figure \ref{mpi100M}, right panel), 
allows us to perform a more extensive scalability test, 
exploring the range between 16 and 128 processors. This test confirms that parallel
{\tt Splotch} can process about 1 particle per microsecond per process. For this data set the scalability
is demonstrated up to 128 processors in the ST mode. The SMT configuration,
while producing the best absolute performances, seems to have a more limited scalability. 
This requires more investigation, but our anticipation is that the most likely reason for this is processor 
architectural features rather than our code.

The pure binary and the MPI readers lead to similar performances. This is due to the features 
of our data set, which is characterized by one dimensional data arrays. This means
that the parallel reading functions can read large chunks of contiguous data 
in a single operation. However improvements due to MPI2 functions appear when 
processes read large chunks of data. So good performances are expected as size of data increases. 
Moreover when the number of processes is high, the MPI I/O reader performs and scales 
better than the other one. Further improvements should emerge when multidimensional 
arrays are considered and the access data pattern is more complex. 
In these cases, collective MPI I/O reading could provide an effective speed-up for data loading.

\subsection{CUDA Benchmarks}

\begin{table}
\caption{The performance timings (secs) obtained with our CUDA implementation of Splotch (indicated by yes) compared to timings obtained using the standard sequential implementation of Splotch (indicated by no) for benchmark data sets 1M and 10M.}
\begin{center}
\begin{tabular}{|l|l|l|l|l|}
\hline
	& setup/read & 	display & 	write & 	total \\
\hline
1M (no) & 	1.1259 & 	0.6982 & 	0.1860 & 	2.0103 \\
\hline
1M (yes) & 	1.0916 & 	0.8411 & 	0.1840 & 	2.1170 \\
\hline
10M (no) & 	10.7064 & 	6.1376 & 	0.1842 & 	17.0284 \\
\hline
10M (yes) & 	10.1411 & 	4.8261 & 	0.1865 & 	15.1537 \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{table}
\caption{The performance timings (secs) obtained with our CUDA implementation of Splotch. 
The number of particles in the data sets employed for our experimental results is
370,852 (small), 2,646,991 (medium) and 16,202,527 (large) respectively.}

\begin{center}
\begin{tabular}{ | l || l | l || l | l | p{3cm} |}
\hline  
   &	CPU	& CPU	& GPU	& GPU	& GPU \\
\hline
  data set & disp. t. & tot. t. & disp. t. & tot. t. & disp. t. no split \\
\hline  
  small &	12.6419	& 13.0144	& 7.3885	& 7.7668	& 12.4894 \\
\hline
  medium &	18.5443 &	19.7190 &	11.3855 &	12.7118 &	16.6905 \\
\hline
  large	& 31.8976 &	39.3234 &	19.7792 &	27.0321	& 24.6305\\
\hline
\end{tabular}
\end{center}
\end{table}

The performance timings for benchmark data sets 1M and 10M using our CUDA
implementation under the Win configuration described earlier are presented in Table 2. 
The overall performance for data set 1M is somewhat degraded when CUDA is employed.
The main reason is the fact that for data set sizes up to this order of magnitude the
costs associated with standard CUDA operations (e.g.\ initializing CUDA runtime 
or data copying from host to device and vice-versa) are non-negligible. 
As the size of our benchmark data set becomes an order of magnitude larger,
using CUDA results in 21.3\% performance gains\footnote{The performance gains 
are computed by: Gain = (TimeWithoutCUDA - TimeWithCUDA) / TimeWithoutCUDA}. 
Our anticipation is that further increasing the order of magnitude of a data set's 
size would result in further performance gains. Our initial experiences with several 
other data sets so far indicate that our CUDA implementation offers maximum gains
when there is a large display calculation involved, that is when individual particles influence relatively large areas on the screen. This is indeed the case for typical display scenarios (see Figure \ref{cudafig}). Table 3 shows that our CUDA parallelization improves the 
performance of Splotch. The 'split particles' strategy (see section 3.2) further 
improves performance timings.
 
%\begin{figure}
%\begin{center}
%\includegraphics[width=0.5\textwidth]{t_read.pdf}
%\end{center}
%\caption{Shown is the scaling of the allclock time needed to read the needed data 
%(position, velocity and smoothing length) for the 10 billion particles of the simulation
%as function of parallel reading tasks. The dashed line indicates the expectation for an 
%ideal scaling. The horizontal lines indicate time expected for XXX and IO throughput of 300 Mbyte/sec,
%1 Gbyte/sec and 3 Gbyte/sec. The read datapoint indicate the reading speed obtained if naively 
%individual data elements are streamed instead of reading large blocks in juncks.}\label{read_scaling}
%\end{figure}

\subsection{Millennium II Visualization}
\label{mII}

The visualization of the outputs of the Millennium II simulation \cite{2009MNRAS.398.1150B} 
represents our most challenging benchmark as it requires the processing of 10 billion particles 
simultaneously. To do this at least 300 GB of memory are necessary, only available 
on HPC platforms such as the SP6 system.
In our performance results we have used the final output of the simulation, producing 
high resolution images of 3200x3200 pixels. A minimum number of 128 processors 
is necessary to process the entire data set.

For this benchmark, we could not use our MPI reader, due to a serious limitation related 
to the MPI I/O API (usage of 32 bits counters which do not allow us to handle the large-scale data sets of the Millennium).
We will work to overcome this problem in the future versions of Splotch. 
At the moment, we used instead the built-in parallel reader derived by the original Gadget 2 code 
(used for the simulation - \cite{gadget}), 
which allows to access directly data files as they where saved by the simulation code. 

Examples of the Millennium II simulation rendered images are shown in Figure \ref{mil2}.
In figure \ref{cpu_scaling}, we present the results of the benchmarks, in ST and SMT modes.
The left panel shows the data processing time from 128 to 2048 MPI threads in ST mode and from
256 to 4096 threads for SMT. The right panel shows instead the data read time, using 
the Gadget 2 reader. In this case, the read process is performed by a subset of processors,
belonging to a dedicated MPI sub-communicator, which then scatters the loaded data to the complete
processors pool. The tests confirm the scalability properties of the code even when a large number
of threads is used. The reader's curve tends to flatten toward 100 processors, achieving 
the physical bandwidth limit of the underlying GPFS based storage system. 

\begin{figure}
\begin{center}
\includegraphics[width=0.8\textwidth]{cu_images.png}
\end{center}
\caption{Sample renderings of small (left), medium (middle) and large (right) data sets.}\label{cudafig}
\end{figure}

\section{Summary and Future Developments}
\label{conclusions}

%Nowadays visualization offers powerful instruments for exploring rapidly in an intuitive and 
%effective way modern and highly complex, large-scale astrophysical data sets from 
%real-world observations or numerical simulations. To overcome the shortcoming 
%of traditional exploration and discovery tools a new generation of software packages 
%is gradually emerging giving the astronomical community robust instruments in the context 
%of massively-large data sets founded on high performance architectures, interoperability
%and collaborative workflows. Our previously developed {\tt Splotch} algorithm 
%can generate effective visualizations of large-scale astrophysical data sets coming from
%particle-based computer simulations. N-Body simulations constitute prime examples 
%of particle-based simulations typically producing large-scale data sets, e.g.\ the
%Millennium II \citep{2009MNRAS.398.1150B}. 

In this paper we have described a high performance parallel implementation of {\tt Splotch} 
able to execute on a variety of high performance computing architectures. This is due 
to its hybrid nature exploiting multi-processor systems adopting an MPI based approach,
multi-core shared memory processors exploiting OpenMP, and modern CUDA enabled graphics 
boards. This allows to achieve extremely high performance overcoming the typical memory 
barriers posed by small personal computing systems, commonly adopted for visualization. 
Finally, as parallel Splotch is implemented in ISO C++ and is completely self-contained
(in other words it does not require any complementary library apart from MPI, OpenMP and CUDA), 
the code is highly portable and compilable over a large number of different 
architectures and operating systems. We discussed test results based on 
custom-made benchmark data sets and also the Millennium II simulation, which is the
largest cosmological simulation currently available containing 10 billion particles.

Our future work will involve porting and running the parallelized {\tt Splotch} on hybrid 
architecture computing systems containing several multiprocessor computers with CUDA 
enabled graphics boards, thus exploiting MPI and CUDA simultaneously. 
Several optimizations are also planned for our CUDA implementation, e.g.\ unrolling 
short loops for improved control flow or using local/shared memory to accelerate data fetching. 
Mechanisms for optimal load balance between CPU and the graphics processor and between several 
graphics processors when these are available should also be considered. We will 
investigate possibilities for designing advanced scheduling mechanisms to minimize 
the idle CPU times contained in the current implementation. Finally, we will explore the 
opportuinities offered by the OpenCL library, in order to exploit a wider range of 
computing architectures. 


\section*{Acknowledgments}
We would like to thank Mike Boylan-Kolchin for providing the MilleniumII simulation data sets 
on which we conducted our performance tests. 
Klaus Dolag acknowledges the support 
by the DFG Priority Programme 117. This work was also supported by the promising 
researcher's award, University of Portsmouth, and the HPC-EUROPA 2 (project number 228398) 
under the EC Coordination and Support Action Research Infrastructure Programme in FP7 
for the committed resources. Martin Reinecke is supported by the
German Aeronautics Center and Space Agency (DLR), under program 50-OP-0901, funded by the
Federal Ministry of Economics and Technology.


\begin{figure}
\begin{center}
\includegraphics[width=0.49\textwidth]{t_cpu.pdf}
\includegraphics[width=0.49\textwidth]{t_read.pdf}
\end{center}
\caption{Shown is the scaling of the CPU time with the number of MPI threads used 
for vizualizing the
final output of the Millennium II simulation. 
The left panel shows the total wallclock time substracting the time needed for 
reading and writing, the right panel the read-data time. 
The dashed line indicates the expectation for an ideal scaling. The test was 
performed on a {\it Power6} architecture. The diamonds represent runs in ST mode, 
the triangles indicate the SMT configuration.
}\label{cpu_scaling}
\end{figure}

\section*{References}


%% References with BibTeX database:
\begin{thebibliography}{00}

\bibliographystyle{elsarticle-num}
\bibliography{master.bib}

%% Authors are advised to use a BibTeX database file for their reference list.
%% The provided style file elsarticle-num.bst formats references in the required Procedia style

%% For references without a BibTeX database:



\bibitem{sdss} http://www.sdss.org/

\bibitem{lofar} http://www.lofar.org/

\bibitem{2009MNRAS.398.1150B}http://www.mpa-garching.mpg.de/galform/millennium-II/index.html

\bibitem{fraedrich2009}R. Fraedrich, J. Schneider, R. Westermann, 
IEEE Transactions on Visualization and Computer Graphics, vol. 15, no. 6, pp. 1251-1258, 
Nov./Dec. 2009, doi:10.1109/TVCG.2009.142

\bibitem{Szalay2008}T. Szalay, V. Springel and G. Lemson, http://arxiv.org/abs/0811.2055, 2008

\bibitem{lsst}http://www.lsst.org/lsst

\bibitem{iraf} http://iraf.noao.edu/

\bibitem{midas} http://www.eso.org/sci/data-processing/software/esomidas/

\bibitem{sao} http://tdc-www.harvard.edu/software/saoimage.html

\bibitem{gaia} http://astro.dur.ac.uk/~pdraper/gaia/gaia.htx/index.html

\bibitem{gnuplot} http://www.gnuplot.info/

\bibitem{supermongo} http://www.astro.princeton.edu/~rhl/sm/sm.html

\bibitem{idl} http://www.ittvis.com/

%\bibitem{survey} XXX

\bibitem{paraview} http://www.paraview.org/

\bibitem{aladin} http://aladin.u-strasbg.fr/

\bibitem{topcat} M. B. Taylor, TOPCAT and STIL: Starlink Table/VOTable Processing Software, ASP, ASPC 347 29T, 2005.

\bibitem{visivo1}M. Comparato, U. Becciani, A. Costa, B. Garilli, C. Gheller, B. Larsson and J. Taylor, 
The Publications of the Astronomical Society of the Pacific, Volume 119, 898-913, 2007.

\bibitem{3dslicer} M. A. Borkin, N. A. Ridge, A. A. Goodman and M. Halle, astro-ph/0506604, 2005.

\bibitem{splash} http://users.monash.edu.au/~dprice/splash/index.html

\bibitem{visit} https://wci.llnl.gov/codes/visit/

\bibitem{simbad} http://simbad.u-strasbg.fr/simbad/

\bibitem{vizier} http://cdsarc.u-strasbg.fr/viz-bin/VizieR

\bibitem{visivo2}U. Becciani, A. Costa, V. Antonnuccio-Delogu, G. Caniglia, M. Comparato, 
C, Gheller, Z. Jin, M. Krokos, P. Massimino, The Publications of the Astronomical Society of the Pacific, 
Volume 122, 119-130, 2010

\bibitem{2008NJPh...10l5006D}K.Dolag, M. Reinecke, C.Gheller, S. Imboden, 
New Journal of Physics, Volume 10, Issue 12, pp. 125006, 2008.

\bibitem{mpi} http://www.mpi-forum.org/

\bibitem{openmp} http://openmp.org/

\bibitem{cuda} http://www.Nvidia.com/object/cuda\_home.html

\bibitem{1985A&A...149..135M} J. Monaghan and J. Lattanzio, A\&A, 149, 135, 1985

\bibitem{1991par..book.....S} Physics of Astrophysics: Volume I Radiation, Shu F., 1991,
Published by University Science Books, 648 Broadway, Suite 902, New York, NY 10012

\bibitem{rvn} http://ibm-deep-computing-visualization-rvn-end.software.informer.com/

\bibitem{cudaprogguide} Nvidia CUDA Programming Guide, Version 2.1 Beta, 10/23/2008, http://www.Nvidia.com

\bibitem{gadget} Springel V 2005 Monthly Notices of the Royal Astronomical Society 364, 1105-1134

\end{thebibliography}

\end{document}

%%
%% End of file `procs-template.tex'.
