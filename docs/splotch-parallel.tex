\input{splotch_include.tex}

\title{High-Performance Astrophysical Visualization Using Splotch}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

\author{Zhefan Jin, Mel Krokos}
\ead{Zhefan.Jin@port.ac.uk, mel.krokos@port.ac.uk}
\address{School of Creative Technologies, University of Portsmouth, Winston Churchill Avenue, Portsmouth, United Kingdom}

\author{Marzia Rivi, Claudio Gheller}
\ead{m.rivi@cineca.it, c.gheller@cineca.it}
\address{CINECA, Via Magnanelli 6/3, Casalecchio di Reno, Italy}

\author{Klaus Dolag and Martin Reinecke}
\ead{kdolag@mpa-garching.mpg.de, martin@mpa-garching.mpg.de}
\address{Max-Planck-Institut f\"ur Astrophysik, Karl-Schwarzschild Strasse 1, Garching bei M\"unchen, Germany}

%\author{Martin Reinecke}
%\ead{martin@mpa-garching.mpg.de}
%\address{Max-Planck-Institut f\"ur Astrophysik, Karl-Schwarzschild Strasse 1, Garching bei M\"unchen, Germany}
%
%\author{Claudio Gheller}
%\ead{c.gheller@cineca.it}
%\address{CINECA, Via Magnanelli 6/3, Casalecchio di Reno, Italy}
%
%\author{Mel Krokos}
%\ead{mel.krokos@port.ac.uk}
%\address{School of Creative Technologies, University of Portsmouth, Winston Churchill Avenue, Portsmouth, United Kingdom}

\begin{abstract}

Current technological advances impact profoundly on the unprecedented growth 
in the quality and quantity of astrophysical datasets. The main characteristic 
is extremely large sizes (many GBs) while forthcoming next-generation astrophysical 
datasets are expected to exhibit massively large sizes (many TBs). Visual data 
exploration and discovery tools are robust instruments for rapidly and intuitively 
inspecting very large-scale datasets to identify regions of interest within 
which to apply time-consuming algorithms.  This paper reports on recent developments 
for a high performance implementation of {\tt Splotch}, our previously developed 
ray-tracing algorithm for effective visualization of large-scale astrophysical 
datasets coming from particle-based simulations. We discuss several approaches for 
paralellizing Splotch to suit multicore CPUs and CUDA-enabled GPUs. 
We present benchmarks of our implementations, with a special focus on the 
Millenium II simulation. Finally we summarise the work concluding with pointers 
to future developments.

\end{abstract}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword


Visual Discovery \sep Splotch \sep Large-Scale Particle-Based Numerical Simulations\sep High-Performance Visualization \sep OpenMP \sep CUDA-enabled GPUs\sep Millenium II Simulation 


%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%%
%% Start line numbering here if you want
%%
% \linenumbers

\section{Introduction}
\label{intro}

Nowadays the technological advances in instrumentation and computing 
capability impact profoundly on the dramatic growth in the quality and 
quantity of astrophysical datasets obtained from observational instruments, 
e.g.  sky surveys \cite{sdss}, \cite{lofar}, or large-scale numerical 
simulations, e.g. the Millennium II simulation \cite{2009MNRAS.398.1150B}.

The main characteristic of modern astrophysical datasets is extremely large 
sizes (in the order of hundreds of GBs) requiring storage in extremely 
large-scale distributed databases. The forthcoming next-generation astrophysical 
datasets are expected to exhibit massively large sizes (in the order of hundreds 
of TBs), e.g. \cite{lsst}.

To obtain a comprehensive insight into modern astrophysical datasets, astronomers 
employ sophisticated data mining algorithms often at high computational costs. 
Visual data exploration and discovery tools are then exploited in order to rapidly 
and intuitively inspect very large-scale datasets to identify regions of interest 
within which to apply time-consuming algorithms. Such tools are based on a 
combination of meaningful data {\it visualizations} and user interaction with them.  

An effective visualizations based apporoach
can be a very intuitive and ready way of discovering and 
understanding rapidly brand new correlations, similarities and data patterns. 
For on-going processes, e.g. a numerical simulation in progress, advanced 
visual data exploration and discovery allow constant monitoring and - if 
anomalies are discovered - prompt correction of the run, thus saving valuable 
time and resources. For such reasons the astronomical community has always 
dedicated special attention to advanced graphical/visualization tools driving 
their evolution or even being directly involved in developing them.

The data exploration tools traditionally employed by astronomers 
are limited either 
on processing and displaying of 2D images (e.g. IRAF \cite{iraf}, by NOAO; 
ESO-MIDAS \cite{midas} by the European Southern Observatory; SaoImage \cite{sao} 
by the Smithsonian Astrophysical Observatory and GAIA \cite{gaia} by ESO) or on 
generation of meaningful 2D and 3D plots (e.g. Gnuplot \cite{gnuplot}; 
SuperMongo \cite{supermongo} and IDL \cite{idl} by ITT Visual Information). 
There is a plethora of other tools, for details the reader is referred to 
dedicated literature surveys.
%, e.g. \cite{survey}.

To overcome the shortcomings of traditional tools, a new generation of software 
packages is now emerging providing astronomers with robust instruments in the context 
of large-scale astrophysical datasets, e.g. Paraview \cite{paraview}, Aladin \cite{aladin}, 
Topcat \cite{topcat}, VisIVO \cite{visivo1} and VisIVO Server \cite{visivo2}, 
3D Slicer\cite{3dslicer}, Splash \cite{splash} and VisIT \cite{visit}. 
The underlying principles are exploitation of high performance architectures 
(i.e. multicore CPUs and powerful graphics boards), interoperability (i.e. allowing 
different applications, each specialized for different purposes, to operate on 
shared datasets) and collaborative workflows (i.e. permitting several users to 
work simultaneously for exchanging information and visualization experiences).

%ParaView \cite{paraview} was developed specifically for handling large-scale 
%datasets efficiently by exploiting distributed memories. Aladin \cite{aladin} is 
%an interactive sky atlas for visualization of astronomical images supporting 
%access to astronomical databases (such as Simbad \cite{simbad} and VizieR \cite{vizier}). 
%TopCat \cite{topcat} is an application that includes several tools for analysis 
%and visualization of astronomical tables. VisIVO \cite{visivo1} is an integrated 
%suite of tools and services specifically designed for multidimensional visualisation. 
%VisIVO Server is a recent development for visualization of large-scale astrophysical 
%datasets \cite{visivo2}.
%
%Other examples of new-generation software packages for astrophysical visualization 
%are 3D slicer \cite{3dslicer} capable of displaying volumetric datasets and user 
%defined cut throughs, SPLASH \cite{splash} specifically designed in the context 
%of astrophysical simulations and VisIt \cite{visit} which is a parallel visualization
%and data analysis application supporting a rich set of visualisation features, 
%e.g. displaying of scalar and vector fields defined on 2D or 3D structured 
%and unstructured meshes. 

This paper describes a high performance implementation of 
{\tt Splotch} \citep{2008NJPh...10l5006D}, our previously developed ray-tracing 
algorithm for effective visualization of large-scale astrophysical datasets coming 
from particle-based computer simulations. N-Body simulations constitute 
prime examples of particle-based simulations, typically associated with very 
large-scale datasets, e.g. the Millennium II simulation \citep{2009MNRAS.398.1150B}. 
This is a simulation of the evolution of a meaningful fraction of the universe 
by means of 10 billion fluid elements ({\it particles}) interacting with each other 
through gravitational forces. The typical size of a snapshot of the Millennium II 
simulation is about 400 GBs representing a particle's ID, position, velocity 
together with additional properties, e.g. local smoothing length, density and 
velocity dispersion. For further details on the Millennium II simulation the reader 
is referred to \citep{2009MNRAS.398.1150B}.

%The fundamentals and the traditional sequential operation of {\tt Splotch} 
%\cite{2008NJPh...10l5006D} are reviewed in section 2. Section 3 discusses 
%our strategy for paralellizing Splotch based on different approaches that 
%are suitable for a variety of underlying architecture configurations. 
%Our implementations are Single Instruction Multiple Data (SIMD) designs 
%founded on the MPI library \cite{mpi} in order to support distributed multicore 
%CPUs, the OpenMP \cite{openmp} for supporting shared memory nodes and finally 
%CUDA \cite{cuda} for exploiting not only currently available but also forthcoming 
%next-generation multiple GPUs. The advantage of adopting several parallelization 
%solutions is that we can deploy them simultaneously on hybrid architectures, 
%e.g. mixed hardware architectures consisting of a large number of multicore CPUs 
%and CUDA-enabled GPUs. Benchmarks for our paralellization designs and a discussion 
%on the Millenium II visualization are presented in section 4. Finally section 5 
%outlines a summary of our work and includes pointers to future developments.


\section{The Splotch Algorithm}
\label{splotch}

\begin{figure}
\begin{center}
\includegraphics[width=0.40\textwidth]{millenium2_veldisp.pdf}
\includegraphics[width=0.40\textwidth]{millenium2_vel.pdf}
\end{center}
\caption{A visualization  of the MilleniumII simulation \citep{2009MNRAS.398.1150B}. The color transfer function uses a particle's velocity dispersion (left) and 3D velocity (right). A close-up of one of the richest structures in the simulation is displayed in the inlay.}\label{mil2}
\end{figure}

The rendering algorithm of {\tt Splotch} is generally designed to handle
point-like particle distributions. Such tracer particles can be smoothed
to obtain a continuous field, which is rendered
%, most commonly the $B_2$-Spline \cite{1985A&A...149..135M}
%\begin{equation}
%   W(x,h)=\frac{8}{\pi h^3}\left\{\begin{array}{ll}
%      1 - 6 \left(\frac{x}{h}\right)^2 + 6 \left(\frac{x}{h}\right)^3 \;\;& 0 \le \frac{x}{h} < 0.5 \\
%      2 \left(1 - \frac{x}{h}\right)^3                              & 0.5 \le \frac{x}{h} < 1 \\
%      0                                                             & 1 \le \frac{x}{h} \\
%   \end{array} \right. , \label{SPH:kern}
%\end{equation}
%where $h$ is the local smoothing length, which is typically defined in a way
%that every particle overlaps with $\approx 32$ neighbors. Therefore, the rendering is based
on the following assumptions:

\begin{itemize}
\item
The contribution to the matter density by every particle can
be described as a Gaussian distribution 
$\rho_p(\vec r)=\rho_{0,p}\exp(-r^2/\sigma_p^2)$.
%\footnote{Note that the
%$b_2$-Spline kernel used in SPH has a shape very similar to the Gaussian distribution.}
In practice, it is much more handy to have a compact support of the
distribution, and therefore the distribution is set to zero at a given
distance of $f\cdot\sigma_p$. 
%Following the approach often used to vizualize 
%cosmological simulation we choose $f$ in such a way that
%$f\cdot\sigma_p$ is related to the smoothing length $h$, i.e.
%to fulfill $h \approx f\cdot\sigma_p$. 
Therefore rays passing
the particle at a distance larger than $f\cdot\sigma_p$ will be practically
unaffected by the particle's density distribution.

\item 
We use three ``frequencies'' to describe the red, green and blue
components of the radiation, respectively. These are treated independently.

\item
The radiation intensity $\bf{I}$\footnote{Here we treat all
intensities as vectors with r,g and b components.} along a ray through the simulation
volume is modeled by the well known radiative transfer equation
\begin{equation}
\frac{d\bf{I}(x)}{dx}=(\bf{E}_p-\bf{A}_p\bf{I}(x))\rho_p(x),
\end{equation}
which can be found in standard textbooks \cite{1991par..book.....S}.
Here, $\bf{E}_p$ and $\bf{A}_p$ describe the strength of radiation emission and absorption
for a given particle for the three rgb-colour components. In general it is recommended to
set $\bf{E}_p=\bf{A}_p$, which typically produces visually appealing images.
%; for special
%effects, however, independent emission and absorption coefficients can be used. Specially typically
%a small gray component (e.g. a small, constant addition to the rgb-components) can be added 
%to the absortpion of each particle ($\bf{A}_p$) to improve the three dimensional impression.
%In general the coefficients can vary between particles, and are typically chosen as a function 
%of a characteristic particle property. 
If a scalar quantity is choosen (e.g. the particle temperature, 
density, velocity dispersion, etc.), the mapping to the three components of $\bf{E}$ and $\bf{A}$ (for red, green and blue)
is typically achieved via a transfer function, realized by a colour look-up table or palette, which can
be provided to the ray-tracer as an external file to allow a maximum of flexibility. If a
vector quantity is chosen (r.g. velocity, magnetic field, etc.), the three components of the vectors
can be maped to the three components of $\bf{E}$ and $\bf{A}$ (for red, green and blue). In addition 
to the color, the intensity of each particle can be also modulated proportionally to another
scalar property (e.g. density, etc.).
\end{itemize}


%Assuming that absorption and emission are homogeneously mixed it
%can be shown that, whenever a ray traverses a particle, its intensity
%change is given by
%\begin{equation}
%\label{i_change}
%\bf{I}_{\mbox{after}}=(\bf{I}_{\mbox{before}}-\bf{E}_p/\bf{A}_p)
%\exp(-\bf{A}_p\int_{-\infty}^\infty\rho_p(x)dx) + \bf{E}_p/\bf{A}_p
%\end{equation}
%The integral in this equation is given by
%$\rho_{0,p}\sigma_p\exp{(-d_0^2/\sigma_p^2)}\sqrt{\pi}$, where $d_0$
%is the minimum distance between the ray and the particle center.
%
%Under the assumption that the particles do not overlap, the intensity
%reaching the observer could simply be calculated by applying this formula to
%all particles intersecting with the ray, in the order of decreasing distance
%to the observer. In reality, of course, the particles do overlap, but since
%the relative intensity changes due to a single particle can be assumed to be
%very small, this approach can nevertheless be used in good approximation.

Further details on the Splotch rendering algorithm can be found in \citep{2008NJPh...10l5006D}. 
Figure \ref{mil2} shows a vizualisation example of a large simulation 
containing 10 billion particles.

\section{Parallel Implementation}
\label{parallel}

The {\tt Splotch} algorithm operational scenario consists of a number of 
stages summarized as follows:

\noindent 1. read data from one or more files;

\noindent 2. process data (e.g. for normalization);

\noindent 3. display data;

\noindent 4. save the final image.

All these steps can be parallelized according to the same strategy using a 
Single Instruction Multiple Data (SIMD) approach. This involves distributing the data 
in a balanced way between the different computing elements. Each computing element 
performs the same operations on its subset of data contributing to the final (unique) imaging. 

The parallel implementation has been achieved using different approaches, 
suitable for different hardware architectures and software environments. 
The MPI library \cite{mpi} has been used to define the overall data and work 
distribution. Data are read in chuncks of the same (or similar) size from each processor 
in the MPI pool. Then the same work is performed for steps 2 and 3 on each chunck by the 
corresponding processor. Finally, the final image is generated and saved only by the root processor
This is, in fact, a light task, which does not require any kind of parallel implementation. 
The work accomplished in steps 2 and 3 can be further split, exploiting 
multicore shared memory processors or modern graphics boards. In the first case, by means of
an OpenMP \cite{openmp} based approach. In the second case, the CUDA \cite{cuda} programming 
environment has been adopted. 

The different parallel approaches can be used separately, 
if the available computing system fits only one of the available configurations, or jointly. 
For instance, on a single core PC with an NVIDIA graphic card, only the CUDA based 
parallelization strategy can be activated and exploited. On a multicore RVN \cite{rvn} 
node both MPI and CUDA can be used. This makes the parallel {\tt Splotch} code 
extremely flexible, portable, efficient and scalable.

\subsection{MPI Implementation}
\label{mpi}

Once the data is properly distributed among the processors, all the remaining operations 
are performed locally and further communication is not needed, until the generation
of the final result. Each MPI process uses the assigned 
data to produce its own partial image. At the end, all the partial contributes are merged by means of a 
collective reduction operation producing the final image. 

The data load stage represents the crucial step for balancing the workload and for fast reading 
data from the disk. 
%In fact, from the results presented in section 4.1, it is clear how,
%as the data size grows,
%the data load process tends to be the most demanding section of the code. 
For this reason,
we have paied specific attention to the effective implementations of this functionality.

The adoption of MPI I/O based functions, represents the ideal solution for obtaining 
a high-performance, scalable read--data utility. 
With this approach each process has a different fileview of a single file 
that allows simultaneous and collective
writing/reading of noncontiguous interleaved data. 
%A view defines what data are visible 
%to each process and 
%consists of: an offset, measured in bytes from the beginning of the file; 
%an elementary type (etype) that 
%defines the unit of data access and positioning within the file; and a template for 
%accessing the file (filetype)
%consisting in a number of etypes and holes (which must be a multiple of the etype size). 
%The basic filetype is repeated again and again, tiling the file, and creating regions of allowed access 
%(where etypes are defined), and no access (where holes are defined).
Our implementation assumes that data are organized in the input file according to a block structure, 
where each block contains a single information for 
all $N$ particles. Therefore we have as many contiguous blocks
as the number $n$ of properties given for each particle, and we can see them as a 2-dimensional array $A$ 
of $n \times N$ float elements. 
Then, we have defined the MPI I/O filetype as a simple 2-dimensional subarray of $A$ 
of size $n \times N/nprocs$.

In order to support high performance computing environments where MPI I/O is not available, 
we have also provided two standard MPI binary readers based on standard cstdio functions.
%that can respectively 
%read binary data files written both in tab and block formats. 
%In both cases 
Data to read are equally distributed among processes and simultaneously each one reads
their own portion of data by a direct access operation. 
%The block reader executes one single read of size $N/nprocs$ for each block of data, 
%while the tabular one reads $N/nprocs$ rows containing all $n$ quantities related to a single particle.
%
In all readers an endianism swap is also managed when required.



\subsection{CUDA Implementation}
\label{cuda}
Nowadays Graphics Processing Units (GPUs) can offer a means of increased performance 
(often substantially) in the context of computationally intensive scientific applications, 
e.g. by exploiting high speed underlying ALUs and stream data-parallel organization. 
The Compute Unified Device Architecture (CUDA) introduced by NVIDIA allows software 
developers full access to highly parallellized modern GPU architectures via a 
simplified C language interface and without demanding previous knowledge of the 
GPU operational scenario details \cite{cuda}. This section presents our experiences 
in developing a CUDA implementation for accelerating the Splotch operation.

A thread on the GPU is extremely lightweight compared to CPU threads, in other 
words changing context among threads is not a costly operation. The minimum 
data chunks processed by a CUDA {\it multiprocessor} are groups of threads called 
{\it warps}. Typically developers handle {\it blocks} containing a large number 
of warps while blocks can be further organized together in {\it grids}. This 
offers the advantage of calling (from the CPU) a function executed on the GPU 
(called {\it kernel}) applied to a large quantity of threads, without worrying about 
the limitations of fixed hardware resources. CUDA will execute the blocks sequentially, 
in case of limited resources but revert to parallel execution for large numbers 
of processing units.  The implication is code that can target simultaneously 
entry-level, high-end or even forthcoming next generation GPUs. The CUDA software 
development guidelines are discussed in the programming guide \cite{cudaprogguide} 
extensively.

%Although CUDA threads are scheduled for execution in terms of groups of warps, 
%threads in individual warps are executed in a Single Instruction Multiple Thread 
%(SIMT) way. Consequently assuming that a data-dependent conditional branch exists, 
%the execution of the relevant branch paths is effectively serialized. As an example 
%suppose that a warp contains 32 threads, the data-dependent conditional branch 
%involves paths A and B and 16 threads follow each path respectively. Consider the 
%situation in which the underlying multiprocessor executes A prior to B, then none of 
%the B threads will be executed until the A threads are completed. This situation 
%can cause a substantial slow-down in overall execution times. The worst-case 
%scenario may even involve only a single thread, the CUDA multiprocessor effectively 
%acting then as a single processor.


The CUDA approach can combine to the MPI parallelization strategy, outlined in 
section 3.1, as soon as data are loaded in the memory of each processing unit (CPU+GPU).
At that point, in fact, units can be regarded as independent from each other, and 
CUDA can be exploited as follows. 

%As there typically exists a significant number of data-dependent conditional branches 
%in the execution of Splotch, the MPI parallelization strategy outlined in section 3.1 
%is unsuitable for CUDA implementation. A further reason is that preserving the image 
%buffer storage requirements followed by the MPI implementation implies less memory 
%at our disposal (from the graphics card's on board memory) for running CUDA threads, 
%thus restricting full usage of the CUDA computational capability. 
%Based on these 
%considerations the overall amount of computation ({\it granularity}) involved in 

Each parallel task of our CUDA implementation is determined by a single particle; in other 
words a single particle is {\it processed} and {\it displayed} by a single CUDA thread. 
The processing involves normalizing (also called ranging) some particle values. 
The displaying involves transformation of particles into screen coordinates, colorization 
for deciding their colours, and finally rendering for determining affected image
areas and combining them together for final imaging.

As modern particle-based simulations contain very large numbers of particles 
(in the order of millions) a large number of CUDA threads can be employed, 
ensuring that underlying multiprocessors can be utilized to their full potential. 
The granularity during the processing, transformation and colorization is more or 
less fixed among different particles. However during rendering the granularity 
can vary considerably as it depends upon the number of screen pixels influenced 
by individual particles. As a worst case scenario consider two particles influencing 
all screen pixels and a single screen pixel respectively. Assuming that these 
particles are handled by the same warp (this is determined by CUDA automatically), 
they are then scheduled to execute simultaneously. This unbalanced granularity 
can compromise significantly overall execution times (see Table 3).

%\begin{figure}
%\begin{center}
%\includegraphics[width=0.50\textwidth]{cu_splotch_process.png}
%\end{center}
%\caption{Overview of our CUDA implementation of Splotch. The GPU runs on the graphics board ({\it device}) while the CPU runs on the main computer ({\it host}). The red arrows represent execution flow of programs on the host and device (8 threads shown) respectively. An empty rectangle indicates a time-slot that a processing unit (device or host) is in idle state.}
%\end{figure}

To alleviate this situation we follow a 'split particles' strategy 
dividing large computational tasks into smaller average ones.  For any particle 
influencing a number of pixels that is larger than a threshold value, the relevant 
computational task is sub-divided into multiple ones, each associated with a subset 
of the original number of pixels. The threshold value 
can be determined in advance and given as input via the parameters configuration 
file employed for running Splotch. To our experience a threshold value close to 
the average of the width and height of the display window works satisfactorily. 
A shortcoming of this approach is that there is of course a computational cost 
involved when doing the splitting. As there is an increased number of particles 
more threads are required for handling them. Execution of the splitting algorithm 
as well as memory copying among host and device involve additional costs. 
Nevertheless our test results demonstrate that our 'split particles' strategy 
improves execution times as illustrated in Table 3.

%The overview of our CUDA implementation for accelerating the operation of Splotch is shown in Figure 2. The red arrows on the graphics board ({\it device}) represent CUDA threads running in parallel, for simplicity 8 threads are shown only. The aforementioned ranging is re-organized in three distinct phases so that global particle information, e.g. min/max values of some attributes, is calculated by the main computer ({\it host}) for improving the overall execution times. The host is also responsible for sorting the particles prior to colorization and moreover executing our 'split particles' algorithm. Finally during rendering the affected screen areas for individual particles are stored in a one-dimensional fragment buffer which is filled in through a render computation on the graphics board. The fragment buffer is then copied to the host which accumulatively combines all relevant fragment buffer computations for final imaging. As the device can be called asynchronously it can effectively operate in parallel with the host, so that render GPU computations and combine CPU operations can be performed simultaneously as shown below.

%\begin{verbatim}
%while  ( not all particles are rendered )
%{
%       find subset S(i) of particle array;
%A:     call device to render S(i);
%       if  ( S(i) is not first subset )
%       { 
%B:          combine with F(i-1), the output of S(i-1) in fragment buffer;
%       }
%C:     copy fragment buffer from device to host;
%       if  ( S(i) is the last subset )
%       {
%             combine with F(i);
%       }
%       i++;
%}
%\end{verbatim}

%The instruction A is the render computation executed on the graphics board in parallel with execution of instruction B, the combination operation carried out by the CPU. The results of A and B are merged by instruction C. Our test results indicate that overall times required for performing combination operations are mostly contained within the times demanded by render computations.

\begin{table}
\caption{The datasets used in the benchmarks for our parallel implementations of {\tt Splotch}}
\begin{center}
\begin{tabular}{|l|l|l|}
\hline
Label & 	Particles& 	Size (Bytes)  \\
\hline
1M   & 	1000000   & 24000000 \\
\hline
10M  & 	10000000  & 240000000 \\
\hline
100M & 	100000000 & 2400000000 \\
\hline
850M & 	882739509 & 21185748216 \\
\hline
M-II & 	10000000000 & 	280000000000 \\
\hline
\end{tabular}
\end{center}
\end{table}

\section{Benchmarks}

The parallelized {\tt Splotch} code has been tested on low-end and high-end hardware 
architectures using several test datasets in order to investigate applicability of 
different parallelization approaches.

Exploiting the high portability of our code we could perform our tests on a 
5000 cores UNIX-AIX  SP6 system (referred to as SP6) and a Windows XP PC (referred to as Win). 
The SP6 system is a cluster of 168 Power6 575 computing nodes with 32 cores per node 
and a memory of 128GB. The Win system is an Intel Xeon X5482 3.2 GHz CPU with 
two NVIDIA Quadro FX 5600 graphics boards. Our target was to test parallelized 
versions of Splotch on computing systems of different sizes and target applications, 
from a standard PC, where small to medium datasets can be used, up to high 
performance platforms for handling highly complex large-scale datasets.

We used several benchmark datasets for our testing as shown in Table 1. The first 
few datasets are derived from a cosmological N-Body simulation of more than 850 millions 
particles characterized by spatial information together with velocities, mass density 
and smoothing length, for defining the size of the region influenced by the properties 
of each particle when deploying Splotch. We randomly extracted datasets containing 
1, 10 and 100 million particles. These are indicated in Table 1 as 1M, 
10M and 100M tests. We also employed the dataset 850M containing the entire simulation. 
Such variety of dataset sizes is required to match the memory requirements of 
the different computing systems available for our testing.  
Our most challenging dataset comes from the Millennium II simulation \cite{2009MNRAS.398.1150B} 
and consists of 10 billion particles.

The data files employed are pure binaries organized in such a way so that different 
quantitites are stored consecutively (e.g. $x$ coordinates of all particles, $y$ coordinates 
of all particles and so on) as expected by the MPI-IO based reader. 

\subsection{MPI Benchmarks}

\begin{figure}
\begin{center}
\includegraphics[width=0.49\textwidth]{bench100M_r.pdf}
\includegraphics[width=0.49\textwidth]{bench870M_r.pdf}
\end{center}
\caption{Scaling of the CPU time (total wallclock time) with the number of MPI threads 
used for vizualising 100 million
and 850 millions particles with an final image size of 800x800 pixels.
Results for binary and MPI readers, in ST and SMT modes are shown}\label{mpi100M}
\end{figure}

Our test results on the SP6 platform for datasets block100M and block850M 
are presented in Figure \ref{mpi100M}.
Power6 cores can schedule for execution two processes (or threads) in the same clock cycle, 
and it is also possible to use a single core as two virtual CPUs. This mode of Power6 
is called Simultaneous Multi-Threading (SMT), to distinguish it from 
the standard Single Thread mode (ST).
Deploying SMT notably improves the performance 
of processing and displaying in {\tt Splotch} by reducing the execution time up to  
the $30\%$. 

The 100M test (Figure \ref{mpi100M}, left panel), allows us 
to run on one processor fitting its memory. 
This is the estimate of the sequential performance of the code. 
The maximum number of processors for this test is set to 8, since the usage of 
more processors is inefficient, due to the small size 
of the data chunks assigned to each processor and the increasing overhead of communication.
On a single processor we get a best performance of about 123 seconds, which means that 
we can load and process approximatively 1 particle per $10^6$ seconds. 
In all cases the code scales almost linearly up to 8 processors, beginning to lose
efficiency between 4 and 8 processors due to the reasons previously explained. 

The larger, 850M test (Figure \ref{mpi100M}, right panel), 
allows us to perform a more extensive scalability test, 
exploring the range between 16 and 128 processors. This test confirms that the parallel 
{\tt Splotch} can process about 1 particle per microsecond. For this dataset the scalability 
is proved up to 128 processors in the ST mode. The SMT configuration, 
while producing the best absolute performances, seems to have a more limited scalability. 
This requires more investigation, but the most likely reason for this is processor 
architectural features rather than our code.

The pure binary and the MPI readers lead to similar performances. This is due to the features 
of our dataset, which is characterized by one dimensional data arrays. This means 
that the parallel reading functions can read large chunks of contiguous data 
in a single operation. However improvements due to MPI2 functions appear when 
processes read large chunks of data. So good performances are expected as size of data increases. 
Moreover when the number of processes is high, the MPI I/O reader performs and scale 
better than the other one. Further improvements should emerge when multidimensional 
arrays are considered and the access data pattern is more complex. 
In these cases, collective MPI I/O reading could provide an effective speed-up for data loading.

\subsection{CUDA Benchmarks}

\begin{table}
\caption{The performance timings obtained with our CUDA implementation of Splotch (indicated by yes) compared to timings obtained using the standard sequential implementation of Splotch (indicated by no) for benchmark datasets 1M and 10M.}
\begin{center}
\begin{tabular}{|l|l|l|l|l|}
\hline
	& setup/read (s) & 	display (s) & 	write (s) & 	total (s) \\
\hline
1M (no) & 	1.1259 & 	0.6982 & 	0.1860 & 	2.0103 \\
\hline
1M (yes) & 	1.0916 & 	0.8411 & 	0.1840 & 	2.1170 \\
\hline
10M (no) & 	10.7064 & 	6.1376 & 	0.1842 & 	17.0284 \\
\hline
10M (yes) & 	10.1411 & 	4.8261 & 	0.1865 & 	15.1537 \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{table}
\caption{The performance timings obtained with our CUDA implementation of Splotch. 
The number of particles in the datasets employed for our experimental results is 
370,852 (small), 2,646,991 (medium) and 16,202,527 (large) respectively. All timings are 
expressed in seconds}

\begin{center}
\begin{tabular}{ | l || l | l || l | l | p{3cm} |}
\hline  
   &	CPU	& CPU	& GPU	& GPU	& GPU \\
\hline
  dataset & disp. t. & tot. t. & disp. t. & tot. t. & disp. t. no split \\
\hline  
  small &	12.6419	& 13.0144	& 7.3885	& 7.7668	& 12.4894 \\
\hline
  medium &	18.5443 &	19.7190 &	11.3855 &	12.7118 &	16.6905 \\
\hline
  large	& 31.8976 &	39.3234 &	19.7792 &	27.0321	& 24.6305\\
\hline
\end{tabular}
\end{center}
\end{table}

The performance timings for benchmark datasets block 1M and block 10M using our CUDA 
implementation under the Win configuration described earlier are presented in Table 2. 
The overall performance for dataset block 1M is somewhat degraded when CUDA is employed. 
The main reason is the fact that for dataset sizes up to this order of magnitude the 
costs associated with standard CUDA operations (e.g. initialization of CUDA runtime 
or data copying from host to device and vice-versa) are non-negligible. 
As the size of our benchmark dataset becomes an order of magnitude larger, 
using CUDA results in 21.3\% performance gains\footnote{The performance gains 
are computed by: Gain = (TimeWithoutCUDA - TimeWithCUDA) / TimeWithoutCUDA}. 
Our anticipation is that further increasing the order of magnitude of a dataset's 
size would result in further performance gains. Our initial experiences with several 
other datasets so far indicate that our CUDA implementation offers maximum gains 
when there is a large display calculation involved, e.g. consider the datasets 
presented in Figure \ref{cudafig}. Table 3 shows that our CUDA paralellization improves the 
performance of Splotch. The 'split particles' strategy (see section 3.2) further 
improves performance timings.

%\begin{figure}
%\begin{center}
%\includegraphics[width=0.5\textwidth]{t_read.pdf}
%\end{center}
%\caption{Shown is the scaling of the allclock time needed to read the needed data 
%(position, velocity and smoothing length) for the 10 billion particles of the simulation
%as function of parallel reading tasks. The dashed line indicates the expectation for an 
%ideal scaling. The horizontal lines indicate time expected for XXX and IO throughput of 300 Mbyte/sec,
%1 Gbyte/sec and 3 Gbyte/sec. The read datapoint indicate the reading speed obtained if naively 
%individual data elements are streamed instead of reading large blocks in juncks.}\label{read_scaling}
%\end{figure}

\subsection{Millennium II Visualization}
\label{mII}

The visualization of the outputs of the Millennium II simulation \cite{2009MNRAS.398.1150B} 
represents our most challenging benchmark as it requires the processing of 10 billion particles 
simultaneously. To do this at least 300 GB of memory are necessary, only available 
on HPC platforms such as the SP6 system.
In our performance results we have used the final output of the simulation, producing 
high resolution images of 3200x3200 pixels. A minimum number of 128 processors 
is necessary to process the entire dataset.

For this benchmark, we could not use our MPI reader, due to a serius limitation related 
to the MPI I/O API (usage of 32 bits counters which do not allow us to handle the huge datasets of the Millennium).
We will work to overcome this problem in the future versions of Splotch. 
At the moment, we used instead a parallel reader derived by the original Gadget 2 code 
(used for the simulation - \cite{gadget}), 
which allows to access directly data files as they where saved by the simulation code, 

Examples of the Millennium II simulation rendered images are shown in Figure \ref{mil2}.
In figure \ref{cpu_scaling}, we present the results of the benchmarks, in ST and SMT modes.
The left panel shows the data processing time from 128 to 2048 MPI threads in ST mode and from
256 to 4096 threads for SMT. The right panel shows instead the data read time, using 
the Gadget 2 reader. In this case, the read process is performed by a subset of processors,
belonging to a dedicated MPI sub-communicator, which then scatter the loaded data to the complete
processors pool. The tests confirm the scalability properties of the code even when a large number
of threads is used. The reader's curve tends to flatten toward 100 processors, achieving 
the physical bandwidth limit of the underlying GPFS based storage system. 

\begin{figure}
\begin{center}
\includegraphics[width=0.8\textwidth]{cu_images.png}
\end{center}
\caption{Sample renderings of small (left), medium (middle) and large (right) datasets.}\label{cudafig}
\end{figure}

\section{Summary and Future Developments}
\label{conclusions}

%Nowadays visualization offers powerful instruments for exploring rapidly in an intuitive and 
%effective way modern and highly complex, large-scale astrophysical datasets from 
%real-world observations or numerical simulations. To overcome the shortcoming 
%of traditional exploration and discovery tools a new generation of software packages 
%is gradually emerging giving the astronomical community robust instruments in the context 
%of massively-large datasets founded on high performance architectures, interoperability 
%and collaborative workflows. Our previously developed {\tt Splotch} algorithm 
%can generate effective visualizations of large-scale astrophysical datasets coming from 
%particle-based computer simulations. N-Body simulations constitute prime examples 
%of particle-based simulations typically producing large-scale datasets, e.g. the 
%Millennium II \citep{2009MNRAS.398.1150B}. 

In this paper we have described a high performance parallel implementation of {\tt Splotch} 
able to execute on a variety of high performance computing architectures. This is due 
to its hybrid nature exploiting multi-processors systems adopting an MPI based approach, 
multi-core shared memory processors exploiting OpenMP, and modern CUDA enabled graphics 
boards. This allows to achieve extremely high performance overcoming the typical memory 
barriers posed by small personal computing systems, commonly adopted for visualization. 
Finally, as parallel Splotch is implemented in ANSI C++ and is completely self-contained 
(in other words it does not require any complementary library apart from MPI, OpenMP and CUDA), 
the code is highly portable and compilable over a large number of different 
architectures and operating systems. We have shown a range of test results based on 
custom-made benchmark datasets and also the Millennium II simulation, which is the 
largest cosmological simulation currently available containing 10 billion particles.

Our future work will involve porting and running the parallelized {\tt Splotch} on hybrid 
architecture computing system containing several multiprocessor computers with CUDA 
enabled graphics boards, thus exploiting MPI and CUDA simultaneously. 
Several optimizations are also planned for our CUDA implementation, e.g. unrolling 
short loops for improved control flow or using local/shared memory to accelerate data fetching. 
Mechanisms for optimal load balance between CPU and the graphics processor and between several 
graphics processors when these are available should also be considered. Finally we will 
investigate possibilities for designing advanced scheduling mechanisms to minimize 
the idle CPU times contained in the current implementation.

\section*{Acknowledgments}
We would like to thank Mike XXX for providing the MilleniumII simulation datasets 
on which we conducted our performance tests. Klaus Dolag acknowledges the support 
by the DFG Priority Programme 117. This work was also supported by the promising 
researcher's award, University of Portsmouth, and the HPC-EUROPA 2 (project number 228398) 
under the EC Coordination and Support Action Research Infrastructure Programme in FP7 
for the committed resources.

\begin{figure}
\begin{center}
\includegraphics[width=0.49\textwidth]{t_cpu.pdf}
\includegraphics[width=0.49\textwidth]{t_read.pdf}
\end{center}
\caption{Shown is the scaling of the CPU time with the number of MPI threads used 
for vizualizing the
final output of the Millennium II simulation. 
The left panel shows the total wallclock time substracting the time needed for 
reading and writing, the right panel the read-data time. 
The dashed line indicates the expectation for an ideal scaling. The test was 
performed on a {\it Power6} architecture. The diamonds are runs using one task per core, 
the triangles indicate the SMT configuration
}\label{cpu_scaling}
\end{figure}

\section*{References}


%% References with BibTeX database:
\begin{thebibliography}{00}

\bibliographystyle{elsarticle-num}
\bibliography{master.bib}

%% Authors are advised to use a BibTeX database file for their reference list.
%% The provided style file elsarticle-num.bst formats references in the required Procedia style

%% For references without a BibTeX database:



\bibitem{sdss} http://www.sdss.org/

\bibitem{lofar} http://www.lofar.org/

\bibitem{2009MNRAS.398.1150B}http://www.mpa-garching.mpg.de/galform/millennium-II/index.html

\bibitem{lsst}http://www.lsst.org/lsst

\bibitem{iraf} http://iraf.noao.edu/

\bibitem{midas} http://www.eso.org/sci/data-processing/software/esomidas/

\bibitem{sao} http://tdc-www.harvard.edu/software/saoimage.html

\bibitem{gaia} http://astro.dur.ac.uk/~pdraper/gaia/gaia.htx/index.html

\bibitem{gnuplot} http://www.gnuplot.info/

\bibitem{supermongo} http://www.astro.princeton.edu/~rhl/sm/sm.html

\bibitem{idl} http://www.ittvis.com/

%\bibitem{survey} XXX

\bibitem{paraview} http://www.paraview.org/

\bibitem{aladin} http://aladin.u-strasbg.fr/

\bibitem{topcat} M. B. Taylor, TOPCAT and STIL: Starlink Table/VOTable Processing Software, ASP, ASPC 347 29T, 2005

\bibitem{visivo1}Visualization, Exploration and Data Analysis of Complex Astrophysical Data,
M. Comparato, U. Becciani, A. Costa, B. Garilli, C. Gheller, B. Larsson, J. Taylor, 2007, 
The Publications of the Astronomical Society of the Pacific, Volume 119, Issue 858, pp. 898-913.

\bibitem{3dslicer} M. A. Borkin, N. A. Ridge, A. A. Goodman and M. Halle, Demonstration of the Applicability of 3D Slicer to Astronomical Data using 13CO and C18O Observations of IC348, astro-ph/0506604, 2005

\bibitem{splash} http://users.monash.edu.au/~dprice/splash/index.html

\bibitem{visit} https://wci.llnl.gov/codes/visit/

\bibitem{simbad} http://simbad.u-strasbg.fr/simbad/

\bibitem{vizier} http://cdsarc.u-strasbg.fr/viz-bin/VizieR

\bibitem{visivo2}VisIVO-Integrated Tools and Services for Large-Scale Astrophysical Visualization, U. Becciani, A. Costa, V. Antonnuccio-Delogu, G. Caniglia, M. Comparato, C, Gheller, Z. Jin, M. Krokos, P. Massimino, The Publications of the Astronomical Society of the Pacific, (to appear).

\bibitem{2008NJPh...10l5006D}Splotch: Visualizing Cosmological Simulations. K.Dolag, 
M. Reinecke, C.Gheller, S. Imboden, 2008, New Journal of Physics, Volume 10, Issue 12, pp. 125006

\bibitem{mpi} http://www.mpi-forum.org/

\bibitem{openmp} http://openmp.org/

\bibitem{cuda} http://www.nvidia.com/object/cuda\_home.html

\bibitem{1985A&A...149..135M} A refined particle method for astrophysical problems, 
Monaghan J.~J. \& Lattanzio J.~C., 1985, A\&A, 149, 135

\bibitem{1991par..book.....S} Physics of Astrophysics: Volume I Radiation, Shu F., 1991,
Published by University Science Books, 648 Broadway, Suite 902, New York, NY 10012

\bibitem{rvn} http://ibm-deep-computing-visualization-rvn-end.software.informer.com/

\bibitem{cudaprogguide} NVIDIA CUDA Programming Guide, Version 2.1 Beta, 10/23/2008, http://www.nvidia.com

\bibitem{gadget} Springel V 2005 Monthly Notices of the Royal Astronomical Society 364, 1105Ë†1134

\end{thebibliography}

\end{document}

%%
%% End of file `procs-template.tex'.
