\documentclass[11pt]{article}
\usepackage{amssymb,amsmath}
\usepackage{graphicx} 

\title{Splotch on GPUs using the CUDA paradigm}
%\author{M. Rivi, C. Gheller, M.Krokos}
\author{.........}

\begin{document}
\maketitle

\section{Introduction}
\label{sec:intro}

The management and analysis of data generated by scientific experiments, 
observations and numerical simulations, currently represent an extraordinary challenge  
both for the research teams who concurred to its production and for 
the data centers, that have to provide technical solutions in terms of 
storage, computing power and software.

The main challenges are due both to the data size, aggregate but also of each 
single dataset, and to its complexity, useful information being hidden in a sea 
of bits. Standard data mining and analysis software often relies on complex
algorithms, that become prohibitively computationally expensive when dealing with 
huge datasets. Visual data exploration and discovery can represent a valuable
support, since they provide a prompt and intuitive insight to
very large-scale data sets to identify regions and/or features of interest within which
to apply time-consuming algorithms. 
Furthermore, this apporoach can be an extremely effective and ready way of discovering 
and understanding correlations,
similarities and data patterns, or to identify anomalus behaviors, that can be
interpreted as a wrong result, avoiding to waist time and effort on the specific dataset or
to save resources in an on-going experiment (e.g. stop a simulation that is producing 
unreliable outputs). Finally, visualization is also an effective means of presenting
scientific results both to experts and to general public.

In order to visualize huge datasets, suitable tools must be available, able to 
exploit High Performance Computing (hereafter HPC) devices, like multicore, multinode
supercomputers, providing suitable resources in terms of computing power, memory size, 
storage capacity and performance and network speed. Currently, not many such tools are avilable.

BIBLIOGRAPHY: visit and paraview, Australian stuff, Tipsy, visivo, stuff from MPI, 3D slicer, Splash

In this paper we focus on {\it Splotch}, our previously developed ray-casting
algorithm. Splotch was born for an effective high performance visualization of large-scale 
astrophysical data sets coming from particle-based computer simulations. The software is 
specialized in the high-quality, high-performance rendering of point like data as those 
produced in cosmology by N-Body numerical experiments, which 
represent prime examples of particle-based simulations. Examples are 
the Millennium ``trilogy'' \cite{millennium}, the Horizon and the
MareNostrum runs \cite{horizon} or the DEUS simulation \cite{deus}, which
are among the most advanced and largest simulations in the field. 
This models reproduce the evolution of a meaningful fraction
of the universe by means of hundreds billion fluid elements, represented as particles,
interacting with each other through gravitational
forces. The typical size of a output (``snapshot'') of these simulations spans from several hundreds 
of Gigabytes up to tens of Terabytes, and typically stores the
ID, position and velocity together with additional properties, e.g.
local smoothing length, density and velocity dispersion, of each particle.

Splotch however, has been successfully adopted also in other application fields,
like the visualization of real galaxy systems, whose 3D shape is carefully modeled
according to observational data. Here, more than the data size, the driver is
the quality and the level of details of the final images,
that have to reproduce the full details of the
spectacular data coming from astronomical observations. Furthermore,
it has been adopted also for the visualization of meshed based astrophysical simulations, 
although the same high quality cannot be achieved unless extremely high resolution
meshes are provided. 

In the development of Splotch, specific care has been taken of all the performance 
issues. The software is optimized in order to require the minimum possible
memory usage and in order to exploit vector architectures, multi-core processors 
and multi-nodes supercomputers \cite{jin:high-performance}. 

In the last years, 
GPUs, and their computational counterpart, GPGPUs, have 
acquired more and more popularity both in the graphics and in the HPC 
communities, since they can provide extraordinary performances on suitable
classes of algorithms, with speed-up factors of about one order of magintude with respect to 
a standard multicore CPU and with a comparable power consumption.
Therefore, on HPC systems, graphic accelerators are becoming more and more common. 
Many supercomputers are equipped with hundreds of GPUs that can overlap 
their computing power to that of the CPUs, strongly reducing the time-to-solution
of many typical scientifc problems.

In order to exploit this additional computing resources, we have implemented 
a GPU version of Splotch. A full refactoring of the code has been necessary 
to get the good performance out of the GPU's implementation. 
All the details are presented in the rest of the paper. In Section 2, we give a short 
overview of the Splotch main algorithms. In Section 3, we recap the main features
of the GPU architecture and the CUDA progemming model
and we present the performance model that drove the 
re-design of the code. In Section 4 
we describe the implemented algorithm. Section 5 shows
the results of the performed tests and benchmarks. Finally, in Section 6, we draw 
the conclusions. 

\section{Splotch overview}
\label{sec:overview}

The main peculiarities of the Splotch software are represented by the 
high quality of the generated images, obtained adopting a specialized ray-casting 
approach, its high performance, due to a strong optimization on HPC architectures and the exploitation of multi-core, multi-node 
systems by means of an effective mix of the OpenMP and MPI parallel programming paradigms, and
the support of large data volumes, through an optimal usage of the memory, with
no data replica or unnecassary allocations, a full 64 bits support and the exploitation
of both shared and distributed memory systems. Furthermore, Splotch's design is kept 
intentionally simple, making its extension to new components and functionalities easy. 
Finally, the software package is completely self-contained, with no dependencies from external 
libraries (apart from those needed for parallelism and HDF5 \cite{hdf5}  for
specific input data file formats). The code is fully C++ and its compilation
usually straightforward, as soon as a suitable makefile script is provided.
No configure procedure is currently supported. The software is fully open source and 
can be obtained in its most recent version contacting the authors.

The Splotch algorithm operational scenario consists of a number of stages. 
\begin{itemize}
\item
{\it Data load}: data is read from one or more data files. A number of different file
formats are supported, from raw binaries, to HDF5 to more specific 
Gadget \cite{gadgeturl} or Tipsy \cite{tipsyurl}
files. The $x$, $y$ and $z$ coordinates are the only compulsory quantities to provide.
These can be cartesian geometric coordinates, but also any other tern 
of variables, adopted to explore a generic three dimensional parameter space.
\item
{\it Rasterization and Processing}: particle coordinates and other geometric quantities 
(like smoothing lengths, see below) are roto-translated and projected according to the 
camera configuration (camera position, look-at direction and orientation). 
Active particles (that actually contributes to the image according to the point of view)
are identified. Active data is normalized, necessary 
transformation (e.g. calculations of logarithms of processed quantities) are performed
and RGB colors (according to read data or to a given
external color table) are associated to each data point. For simplicity, in the rest 
of the paper we will refer to this part of the code as {\it Rasterization} algorithm.
\item
{\it Rendering}: particles contribution to image pixels are calculated.
\item
Image save: final images are saved using the TGA format.
\end{itemize}
The workflow is shown in figure~\ref{fig:workflow}, where the parts of the algorithm parallelized
either with MPI or with OpenMP are emphasized. For a detailed description of
the hybrid parallel implementation of Splotch and its performances, 
we refer to \ref{jin:high-performance}.
In the same figure, also the parts subject to the GPU implementation are shown, 
anticipating the discussion of the following sections.

\begin{figure}
\centering
%\includegraphics[scale=0.5]{Images/workflow.jpg}
\caption{...}
\label{fig:workflow}
\end{figure}

The most peculiar part of the Splotch software is represented by its ray-casting algorithm.
This is also its most computationally demanding component.
In order to effectively handle point-like distributions, the following 
steps are carried out: 

\begin{itemize}
\item
the particle $p$ with coordinates $\vec r$ transports a quantity $\rho_p(\vec r)$ 
(e.g. the mass density or the temperature at position $\vec r$)
that modulates the contribution of the specific particle to the image
according to the following gaussian distribution: 

\begin{equation}\label{smooth}
\rho_p(\vec r)=\rho_{0,p}\exp(-r^2/\sigma_p^2),
\end{equation}

where $\sigma_p$ is the smoothing length, which represents
a sort of size of each particle.  
In principle, each data point contributes to all the pixels of the final image.
In practice, it is much more handy to have a compact support of the
distribution, that is, the distribution is set to zero at a given
distance $\chi\cdot\sigma_p$, where $\chi$ is a proper multiplicative factor.
Therefore rays passing
the particle at a distance larger than $\chi\cdot\sigma_p$ are
unaffected by $\rho_p$.

\item
The red, green and blue color components are calculated independently.

\item
The radiation intensity $\bf{I}$ (treated
as a vector with r, g and b components) along a ray through the simulation
volume is modeled by the well known radiative transfer equation
\begin{equation}\label{rad}
\frac{d\bf{I}(x)}{dx}=(\bf{E}_p-\bf{A}_p\bf{I}(x))\rho_p(x),
\end{equation}
which can be found in standard textbooks \cite{1991par..book.....S}.
Here, $\bf{E}_p$ and $\bf{A}_p$ describe the strength of radiation emission and absorption
for a given particle for the three rgb-colour components. 
%In general it is recommended to
%set $\bf{E}_p=\bf{A}_p$, which typically produces visually appealing images. This is presently a
%necessary setting for Splotch, in order to reduce the complexity of some aspects of its parallel
%implementation. This constraint will be eliminated in the next releases of the code.
If a scalar quantity is chosen (e.g.\ the particle temperature,
density, velocity dispersion, etc.), the mapping to the three components of $\bf{E}$ and $\bf{A}$ (for red, green and blue)
is typically achieved via a transfer function, realized by a colour look-up table or palette, which can
be provided to the ray-tracer as an external file to allow a maximum of flexibility. If a
vector quantity is chosen (e.g.\ velocity, magnetic field, etc.), the three components of the vectors
can be mapped to the three components of $\bf{E}$ and $\bf{A}$ (for red, green and blue). 
In addition
to the color, the optical depth of each particle can be also modulated proportionally to another
scalar property (e.g.\ density, etc.).
\end{itemize}

For further details on the Splotch algorithms we refer to \cite{2008NJPh...10l5006D}.

\section{The GPU Code}
\label{sec:gpu-code}

GPUs represent an efficient tool to improve the performances of a numerical code.
However, algorithms have to be reimplemented in order to run on the accelerator. 
Often, a complete redesign is necessary to achieve good performace and in many
cases such performance is much lower than the nominal one. Nevertheless, even in 
the case of moderate performance improvements, a GPU enabled code allows the 
user to exploit the full power of the majority of the HPC systems currently 
available, which combine usual multicore processors with multiple graphic accelerators.

Splotch has undergone such refactoring process, adopting the CUDA paradigm 
for an effective usage of the GPU. The basic steps accomplished in
the realization of the GPU code, are presented in the next sections. 

%The potential dependency of any pixel of the image from any data point,
%makes the distribution of work between GPU's cores challenging to implement
%and even harder to tune in terms both of performances and of memory requirements.
%In section XXX, we propose a simple performance model that has
%driven the implementation of the GPU enabled Splotch kernels.

\subsection{The CUDA programming model} 
\label{sec:cuda}

For the implementation of the GPU code we have adopted the CUDA 
programming model \cite{cudaurl},
that is currently the standard ``de facto'' for GPU programming.
CUDA is developed by NVIDIA, closely mapping the underlying
hardware (NVIDIA cards), leading to an optimal tuning of the performance.
The limited portability represents its main drawback, somewhat mitigated
by the diffusion of NVIDIA GPUs.
The CUDA standard offers access to highly parallelized modern GPU architectures via a simplified C/C++ or Fortran language interface. It is designed to support joint CPU/GPU execution of an application, where serial sections are performed by the CPU (host), while those which exhibit rich amount of data parallelism, are performed by the GPU (device) as CUDA kernels. The CPU and the GPU have separate memory spaces so data must be transferred from each other via PCI Express 
bus (hereafter PCI-E). 
CUDA kernels launch is asynchronous, so that the host can
 perform the following instructions of the code while the device is computing. If one of them requires the completion 
of the kernel execution, then it is necessary to call the \textit{cudaThreadSyncrhonize}() function before executing it.
A kernel is written for a single thread and instantiated as a grid of many lightweight parallel threads, organized into blocks of the same size. A thread is an indipendent element of work and maps to a hardware core. A block is a 1D, 2D or 3D set of concurrently executing threads that can cooperate among themselves through barrier synchronization and ``fast'' shared memory. 
This is possible because threads belonging to the same block are executed on the same multiprocessor (SM). On the other hand
 synchronization is not possible between blocks of a grid. In fact, the limited amount of memory limits the number of 
blocks that can simultaneously reside in the same SM. Moreover when one block stalls the runtime system switch to 
a different one, hence there is no guaranteed order of execution.

Once a block is assigned to a SM, it is partitioned into 32-thread units called warps. They are scheduling units in SM:
all threads in the same warp execute the same instruction (Single-Instruction, Multiple-Thread). Hence, programmers should minimize the number of execution branches inside the same warp. It is convenient to assign a large number of warps to each SM (i.e. high occupancy), because the long waiting time of some warp instructions is hidden by executing instructions from other warps  and therefore the selection of ready warps for execution does not introduce any idle time into the execution timeline (zero-overhead thread scheduling). 

\subsection{Performance model and analysis}
\label{sec:model}

Splotch's main computational kernels are {\it Rasterization} and {\it Rendering}.
The initial data load phase, though often demanding, will not be treated
here since it is not subject to the GPU implementation. The final save image stage
instead, is not relevant for the performance.

%The main parameters of the model are $N_{part}$, the number of particles
%to be processed, and $N_{pix}$, the number of pixels of the final image
%(in general we can consider square images with $N_{pix}=N_{pix}^2$). For 
%the Splotch's rendering kernel a third parameter must be introduced, 
%that is the average smoothing length of the particles,
%$R_s$. Its value depends both on the intrinsic "size" of the particle
%and on the position of the point of view (particle size increases getting closer to 
%the point of view).
%
%The kernels scale with these parameters as:
%\begin{align}\label{scaling}
%& N_{Norm} \propto N_{part},\\
%& N_{Geom} \propto N_{part},\\
%& N_{Color} \propto N_{part},\\
%%& N_{Sort} \propto N_{part}{\rm log}(N_{part}),\\
%& N_{Render} \propto N_{part} R_s^2,\\
%& N_{Image} \propto N_{pix}^2,
%\end{align}
%where $N_{kernel}$ is the number of operations for a given kernel.

%The Normalization, Geometry and Coloring kernels are  
%perfectly data parallel, each particle being processed independently from the others.
%Thus they are expected to fit the GPU programming model. 
%The Rendering kernel is, in general, the most computationally demanding. 
%The computational time depends in a predictable way with the number of particles and the number 
%of pixels. The most challenging dependecy, however, is from the smoothing length. 
%This dependency leads to strong changes in the computational effort and makes
%the work-load hard to balance, when this is distributed among many-processors
%systems.

In order to model the Splotch algorthm's performances, we split the different timings
required to perform the main computational steps. 
The Splotch's performances can be quantified as time spent on the CPU, time spent on the GPU
and time spent to move data among different memories:
\begin{equation}\label{Ts}
T_{TOT} = T_{cpu} + T_{pci} + T_{gpu} + T_{Mgpu},
\end{equation}
where $T_{TOT}$ is the total time, 
$T_{cpu}$ is the time spent on the CPU, $T_{gpu}$ is the time
spent on the GPU for computation, $T_{pci}$ is the time needed to move data from
the CPU to GPU and back through the PCI-E bus and $T_{Mgpu}$ is the time 
spent in moving data among the global and the shared memory on the GPU.

\medskip
\noindent
{\bf Data transfers}

Data offload to the GPU is one of the crucial steps in the algorithm. 
The minimum amount of data 
to be moved to the GPU is represented by the particles information 
(coordinates, fields, smoothing length). If $S_{part}$ is the
size of a single particle (in the current implementation $S_{part}=35$ bytes) and 
$N_{part}$ is the number of particles to be processed,
$N_{part} S_{part}$ is the total amount of particle data that has to be moved to the device.
The final processed image (the result) is the only data that has to be copied back to
the CPU. Thus, in principle, only $12 N_{pix}^2$ bytes (R, G, B float values) have to be transferred at
the end of the rendering phase. Here, $N_{pix}$ is the linear resolution in pixel of the 
image (we consider square images, the generalization to rectangular images being
straightforward). 

Particles' data offload can be accomplished either in a single or in a few operations, 
depending on the data size (if it fits the available GPU memory) and/or
in order to exploit CUDA streams, which allows to overlap communication 
and computation, hiding the data transfer overhead.  
In order to implement this solution, data has to be
split in chunks and passed to the GPU one after the other, in a loop. Whilst chuncks 
are transferred, computation on previous chuncks, already stored on the GPU
memory, can be carried out. 
The data transfer time can be estimated as:
\begin{equation}\label{pci}
T_{pci} =  N_{chunks} \tau_{pci} + {N_{part} S_{part} + 12 N_{pix}^2 \over 
\mu_{pci}},
\end{equation}
where $\tau_{pci}$ is the transfer time latency (in seconds) and $\mu_{pci}$ is the
bus bandwidth (in bytes per second), $N_{chunks}$ is the number 
of copy stages. 
Equation $\eqref{pci}$ shows that performance
depends linearly both on the number of particles and on the number of pixels.
In the case of large datasets we have that
$N_{part} >> N_{pix}^2$. For instance, the test case adopted
in section~\ref{sec:results} consists in $N_{part} \sim 10^8$
and $N_{pix}^2 \sim 10^6$. In such cases, the contribution of $N_{pix}^2$
to $T_{pci}$ is negligible.

Splitting data in sub-chunks does not
cause any meaningful overhead if the copy latency is kept small, that is:
\begin{equation}
N_{chunks} < {N_{part} S_{part}\over \tau_{pci}\mu_{pci}}.
\end{equation}
For instance, if $N_{part}$ is $O(10^8)$, $N_{chunks}$ can be up to $10^5$ before
contributing to the overhead.

Equations $\eqref{pci}$ considers only the main data tranfers of
the Splotch's algorithm. However, further data tranfers may be necessary.
This aspect will be treated in section~\ref{sec:implementation}.

\medskip
\noindent
{\bf GPU computation}

The time spent in processing the particles on the GPU can be estimated as:
\begin{equation}
T_{gpu} = N_{op}/\nu_{GPU},
\end{equation}
where $\nu_{GPU}$ is the GPU flops/sec rate and
\begin{equation}\label{ops}
N_{op} = N_{part}(\alpha + \gamma R_s^2) + f_{GPU},
\end{equation}
with $\alpha$ and $\gamma$ estimating the number of operation of 
the Rasterization kernel  
and of the Rendering kernel respectively. The function 
$f_{GPU}$ takes into account any GPU specific kernel (i.e. a kernel
which is not present in the original code, necessary to the GPU implementation). 

The parameter $R_s$ is the most peculiar dependency of the Splotch's rendering
algorithm. It is defined as the expectation value of the particles radii:
$R_s = <r_s(p)>$, where
\begin{equation}\label{radius}
r_s(p) = A(p){\chi \sigma_p\over S_{box}} N_{pix},
\end{equation} 
$A(p)$ being the function to tranform to image coordinates,
$\chi$ a proper factor defining the compact support size (see  
equation \eqref{smooth}) and $S_{box}$ the computational box size.
The particle radius depends both on the intrinsic properties of the particle
and on the camera position. The same particle can have a negligible size
from one point of view and it can affect the whole image from a different one.
On the other hand, particles close to each other can have different radii depending
on their intrinsic characteristics. The dependency from $r_s(p)$ poses 
two major problems to the GPU implementation of the rendering 
algorithm. First, two threads acting on different particles could try 
to update the same pixel.
This can cause frequent concurrent accesses to memory
which lead to wrong results, with the correcteness and quality of the resulting image 
degraded if not completely compromised.
Second, different particles can require very different workload, affecting 
a different number of pixels. Therefore, a good load balancing
between threads is very hard to achieve.  
Specific solution must be designed to circumvent such difficulties 
without paying big penalties in terms of performance (as would happen, for instance,
adopting atomic updates, supported by CUDA, but too slow to be used for continuous 
updates of the pixels values).

Finally, it is interesting to stress that $T_{gpu}$ depends linearly on the number of particles. 
This feature should be preserved by the GPU implementation, so the term 
$f_{GPU}$ does not have to introduce any higher order dependency from $N_{part}$. 

\medskip
\noindent
{\bf Memory accesses}

Global-shared memory data transfers can be estimated 
as the number of loads from the global to the shared memories of the available 
streaming multiprocessors plus the number of stores from the shared to the global memory: 
\begin{equation}
N_{Mgpu} = (N_{load,p} + N_{store,p}) N_{part} + N_{store,pix} N_{pix}^2,
\end{equation}
where $N_{load,p}$ and $N_{store,p}$ are the number of load and store of the 
particles, while $N_{store,pix}$ is the number of stores of the image (no loads 
are expected, since the image is created on the GPU). 
Furthermore, memory access overhead can be estimated as $N_{Ngpu} \nu_{mem}$,
where $N_{Ngpu}$ is the total amount of memory accesses, $\nu_{mem}$
is the memory frequency. 
Hence, the time for moving data among memories is:
\begin{equation}\label{tmgpu}
T_{Mgpu} = {(N_{load,p} + N_{store,p}) N_{part} S_{part}
+ 12 N_{store,pix} N_{pix}^2\over \mu_{gpu}}
+ N_{Ngpu} \nu_{mem} + g_{GPU},
\end{equation}
where $\mu_{gpu}$ is the global memory bandwidth
and $g_{GPU}$ the time 
spent on memory accesses by the GPU specific
functions, corresponding to the $f_{GPU}$ term in equation \eqref{ops}.
The terms $f_{GPU}$ and $g_{GPU}$ will be the subject of section~\ref{sec:implementation}.

Equation \eqref{tmgpu} shows that the performance related to global-shared memory 
data transfers
depends on the two parameters $N_{load,p}$ and $N_{store,p}$, that quantify the
total amount of data transferred from the global to the shared memories 
and back, whose performance depends on the memory bandwidth.
The optimal solution would be to have only a single data transfer
($N_{load,p} = 1$ and $N_{store,p} = 0$), with each thread
fully processing a different particle. This {\it one thread per particle} solution guarantees
the best exploitation of the GPU architecture.
This solution, though ideal, is not practical, the main reason being the frequent race conditions
rising from threads trying to concurrently update the same pixels.
Only part of the algorithm is data parallel and can adopt the {\it one thread per particle}
approach. This corresponds to the Rasterization kernel, in which each particle
is processed independently from the others, thus
$N_{load,p} = 1$ and, since processed particles
have to be copied back to the global memory to be rendered, $N_{store,p} = 1$.

The rendering part requires a futher particle data load step, while no more store 
stages are necessary.

The $T_{Mgpu}$ term depends linearly on the image size. Once more, 
for large datasets, this term appears to be negligible. However, since
particles falling outside the field of view are not processed (``inactive'' particles),
depending on the camera position, it can give a meaningful contribution to
the computing time.

A critical term of equation \eqref{tmgpu} is represented by the memory access
latency $N_{Ngpu} \nu_{mem}$. The access to memory is slow in comparison to
the available bandwidth, so a small number of accesses, $N_{Ngpu}$, is crucial
to the performance. This can be achieved by standard caching strategies, that
however are effective only if data contiguity is guaranteed. In other words,
if contiguous particle data moved to the GPU L2 cache can be reused efficiently
by the various threads. This is possible only through a global reorganization 
of the particle data. Such reorganization can however lead to an important
working and memory usage overhead, contributing to the terms $f_{GPU}$ and
$g_{GPU}$ respectively. 

Any other memory access specific to the GPU implementation algorithm,
accounted by the term $g_{GPU}$, must be implemented 
avoiding the introduction of eccessive overheads and preserving the overall
linear scalability with the number of particles.

\medskip
\noindent
{\bf CPU computation}

The final contribution to equation \eqref{Ts} is $T_{cpu}$. In principle this should be negligible, 
the large part of the work being performed by the GPU. Actually, the CPU
can contribute to process a suitable fraction of the particles. Using
CUDA asyncronous operations this work can be overlapped 
to the GPU work, accounting for a fraction of the work needed to 
process the final image.

\section{The CUDA implementation}
\label{sec:implementation}

As soon as data is loaded in the global memory of the GPU, each particle is processed 
by the Rasterization kernel. This kernel can adopt an efficient  
{\it one-thread-per-particle} approach, by which the whole computation on 
a particle (geometric tranformation, coloring etc.) is carried out
by a single thread, with no interaction we other processes.  
This approach leads to a full exploitation of the GPU architecture. 

After Rasterization, particles are ready to be rendered (Rendering kernel).
For the Rendering stage, a simple {\it one-thread-per-particle} approach
cannot be adopted (see section~\ref{sec:model}). Furthermore, memory usage must be carefully managed.

\subsection{Rendering algorithm design}
\label{sec:design}

On the GPU, the rendering algorithm has been designed according to the following procedure.

Particles are first classified in 3 groups according to their size:
\begin{itemize}
\item 
C1: $r_s > r_0$
\item
C2: $0.5 < r_s \le r_0$
\item
C3: $r_s \le 0.5$
\end{itemize}
where $r_s$ is the particle radius in pixels defined by equation \eqref{radius}.

Particles of different classes pose different challenges to the rendering 
algorithm and are processed with different approaches. Particles of class C3 
influence a single pixel, therefore they can be efficiently 
processed with a ``one-thread-per-particle" approach. Setting $r_0 \sim 0.01 N_{pix}$, 
particles of class C2 can influence only small fractions of the image. If 
we divide the image in ``tiles" of suitable size, each C2 particle can affect 
only one tile (see below for details). Finally, particles of class C1 can affect 
a large fraction of the image, spanning multiple tiles.

The particle classification is performed in the Rasterization kernel, with negligible impact
on the computing effort. Each particle is labeled
with a {\it tile index} $n_c$ as follows:
\begin{itemize}
\item 
$n_c = -2$, inactive particle (outside the field of view);
\item
$n_c = -1$, if it belongs to C1 class; 
\item
$0 < n_c < N_t$, if it belongs to C2 class and its centre falls in the $n$-th tile of the image;  
\item
$n_c = N_t$, if it belongs to C3 class, i.e. is point-like.
\end{itemize}
The parameter $N_t$ is the number of tiles the image is divided in. It is defined as
\begin{equation}
N_t = {N_{pix}^2\over t_s^2},   
\end{equation}
where $t_s$ is the tile side in pixels. In the code, this parameter is set to $2r_0$ 
by default but it can be tuned according to the specific use case to tune the performance. 

After this operation, inactive and host particles are removed. Then a new kernel has to be implemented in order to sort the remaining particles by the $n_c$ key and compute the number of those with the same index. 
At the end of this operation, all particles with the same tile index are contiguous in memory, providing an optimal memory access pattern, and can be easily isolated for tiles assignment and processing.

The sorting operation plays a crucial role. It allows to manage particles on the 
device and it is necessary for the efficient execution of further operations like 
reduction and prefix sum by key. In fact, memory accesses to the elements involved 
in a CUDA operation are coalesced when all of them are consecutives. However, sorting 
is intrinsically an expensive operation. Hence, an efficient CUDA implementation of
the sorting function has to be adopted in order to reduce its overhead. 
The Thrust library \cite{thrusturl} provides such function. Thrust is a C++ template library 
for CUDA which mimics the Standard Template Library (STL) and provides 
optimized functions to manage very large arrays of data. In particular, Thrust 
implements a highly-optimized Radix Sort algorithm for sorting primitive types
(e.g., char, int, float, and double) with the standard less comparison operator and 
apply dynamic optimizations to further improve its performance.
Thrust functions provide also effective functions for reduction by key and prefix sum, which are 
increasingly performant on very large arrays.

All these functions scale linearly with the data size (actually the sort scale 
as $kn$ where $k$ is the number of significant key bits),
preserving, in practice, the overall linear dependency of the algorithm with $N_{part}$.

The \textit{tiling scheme} for rendering C2 particles on the device consists in 
assigning particles related to the same image tile to a block of CUDA threads 
and exploit the shared memory and thread synchronization within the block to store 
and compose the image tile. The local size of the tile is defined in order to let any particle belonging to it to be entirely contained in the tile. This is achieved by 
adding to the $body$ of the tile of $t_s^2$ pixels a $boundary$ of $r_0$ pixels around it (see figure XXX). We will refer to this extended tile as a \textit{Btile}.
Particles are accessed in chunks of 256 elements. Each chunk is accessed and stored 
in the shared memory by a single parallel read operation, performed simultaneously by all (or almost all) the threads of the block. Then, the particles of the chunk are rendered sequentially, each with a single parallel operation: each pixel of the particle is processed by a different thread of the block (the pixel number processed by each thread may change as the particle varies, but it remains in the Btile). 
This solution avoids race conditions when composing the image tile, since each 
thread of the same block accesses different pixels. Moreover the workload of each thread is the same even if particles may have different size.

When all particles of the block are rendered, the contribution of the Btile 
is added to the image stored in the global memory. 
Specific care has been taken in avoiding losing contributions coming 
from overlapping regions (boundaries). Since CUDA blocks are order independent, three more copies of the image in the global memory are necessary to store corners, rows and columns of the boundary respectively without race conditions. The final image is then obtained by adding these copies.

Rendering of C3 particles is simpler, since they affect a single pixel. 
Thus, only the position of the particle in the global image has to be calculated.
This can be efficiently carried out by assigning a CUDA thread to each C3 particle. 
However, since different threads can affect the same pixel, their contribution (fragment) cannot be directly added to the image. Hence, this is done in a second step by allocating in the device memory one buffer (index buffer) to store the corresponding ids of the particles. Finally, the image is produced by reducing by key (pixel id) the fragments, that in this case are equal to the colour associated to the particle in the Rasterization kernel. This consists in a sort of the C3 particles by pixel id, followed by the reduce of 
contiguous elements falling in the same pixels, by using the corresponding 
functions provided by the Thrust library.

For both C2 and C3 particles the main drawback of the proposed solutions 
is represented by the overhead due to the sorting and reduction operations. In particular the sorting operation is intrinsically time-consuming.
%CLA: QUA CI VUOLE QUALCHE COMMENTO IN PIU'
Performances of the C2 particles rendering algorithm are also influenced 
by the size of the shared memory, which limits the number of resident blocks
per multiprocessor during the execution of the kernel, thus reducing the theoretical
occupancy. A further issue is related to the
unbalanced work load of each block. In fact the number of particles falling
in each tile can vary in a significant way from one tile to another. 
For C3 particles, a limiting factor is related to the size of the index buffer, but it is quite negligible because each element is an integer number and the number of fragments is equal to the number of C3 particles.
All these aspect will be quantitatively analyzed in section~\ref{sec:results}.

Particles of class C1 are the most challenging to process. Their large 
radius prevents the usage both of a tiles based solution and of a fragment buffer.
In the first case, tiles would be too large to be stored in the shared memory
and, in all cases, their large overlap could lead to strong overheads in the composition of the 
final image. A fragment buffer instead could require too much memory, since 
for each C1 particle a great number of fragments would be generated, possibly
larger than the available memory. The adopted solution circumvented these 
difficulties by copying C1 particles back to the CPU and perform the rendering 
with the original, serial algorithm. This is possible thanks to CUDA asynchronous
operations, which allows to copy data from the device to the host (and vice-versa)
whilst the calculation on the GPU proceeds. Once C1 data are back on the CPU, 
their processing can be performed concurrently to that of the GPU.  
With this approach, we manage to exploit both the host and the device
at the same time, hiding the difficulties related to the rendering of ``big"
particles. This solution is effective as long as the number of C1 particles 
is much smaller than that of particles belonging to the other two classes. 
In the worst case, all the particles would be classified as C1 and the time to solution 
would be of the same order of that of the pure sequential code. 

The final step is the composition of the two partial images (one processed by the
GPU the other by the CPU) into the final result. 
Such operation is performed by the CPU, 
once the GPU partial image has been transferred, with a single copy operation, to
its memory. The data transfer has no impact on the performance, involving just a few 
megabytes of data. The same holds for the reduction of the two partial images.   

\section{Tests and results}
\label{sec:results}

We have analyzed the performances of Splotch's GPU implementation in a number of cases, designed in 
order to stress the different features of the algorithm, 
comparing the results 
obtained using the GPU to those obtained with the original code on the CPU. 

\begin{figure}
\centering
\includegraphics[scale=0.6]{images/box.eps}
\caption{The simulated box. White dots represent stars and galaxies, while the 
diffused blue component is the gas}
\label{fig:box}
\end{figure}

The dataset adopted in all the tests is the result of a medium sized cosmological 
N-body simulation performed using the Gadget code \cite{gadgeturl}. It consists in about 
400 million particles, with about 200 million of dark matter particles, the same amount 
of baryonic matter (herafter ``gas'') and about 10 million star particles. 
All particles are characterized by their spatial coordinates, velocities
and smoothing length. 
Both the gas and the stars have a number of other quantities associated, that can be used 
as color and intensity. Mass density and temperature for the former, spectral type and age
for the latter. Dark matter is not used in our tests. Figure~\ref{fig:box} shows 
the entire data cube visualized by Splotch.

The large size of the dataset, $\approx 7.5$ GB can be handled only by a large memory computing node. 
This is not an issue for the GPU, since data are split 
in smaller chunks and loaded iteratively on the accelerator. 

Our tests have all been performed on a 16-core AMD Opteron 6272 2.1 GHz Interlagos processor,
with 32 GBytes of memory, equipped with one NVIDIA Tesla X2090 GPU with 6 GB of GDDR5 memory,
177 GB/sec 
main memory bandwidth, 665 GFlops/sec of peak performance. GCC 4.3.4 and CUDA 5.0.20 
were used for the code compilation.

\subsection{GPU performance tuning}
\label{sec:gpuperf}

%CLA: QUI METTEREI TUTTO CIO' CHE E' LEGATO AI TEST CHE HAI FATTO PER CUDA

\subsection{Performance analysis}
\label{sec:performance}

The main tests' parameters are the data size ($N_{part}$),
the image size ($N_{pix}^2$) and the distribution of the number of pixels 
per particle ($r_s(p)$) and its expectation value $R_s$.

\medskip
\noindent 
{\bf Scalability with particles and image size}

A first series of tests analyze the scalability of the code
with the size of the processed dataset. In these tests, both the image size
and the camera position are kept fixed and set so that all the particles are processed 
by the GPU.

\begin{figure}
\centering
\includegraphics[scale=0.5]{images/scalan.eps}
\caption{Scalability of different Splotch's kernels on the GPU with the dataset size}
\label{fig:scalability}
\end{figure}

Figure~\ref{fig:scalability} shows different timings, progressively increasing the number of particles
from about $10^7$ up to the total size of about $2.2\times 10^8$. 
The overall computing time scales linearly, 
confirming that the the GPU code preserves the linear dependency from the 
number of particles. 
The Rasterization time always represent 
a minor contribution to the overall time, resulting approximately the 10\% of the rendering 
time. For the original serial implementation, this ratio varies between the 35 and the 60\%.
This proves that the {\it one thread per particle} approach adopted for this kernel
is extremely effective, making its impact on computing time almost negligible.
The ``Rendering'' time 
is due to the proper rendering kernel. It shows a perfect linear scalability
with $N_{part}$ and represents one of the two main contributions
to the computing time, the other being the sum of the overheads related to all those 
parts specific to GPU code refactoring 
(e.g. host-device copy time, particle classification etc.). 
This overhead  will be discussed in more details later in this section.

\medskip
\noindent
{\bf Scalability with radii and comparison with the CPU}

The smoothing radius depends both on the intrinsic properties of the particle
and from the camera position. 
As discussed in section~\ref{sec:model} , this has a big impact 
on the implementation of the GPU rendering kernel and
on the performance. In order to investigate the dependency of the performances from
the smoothing radii, we have permorformed a number of tests progressively 
positioning the camera closer to the dataset center. 
%The particles radii distribution for each
%camera position is presented in figure XXX. 
Approaching the center of the 
data cube, the radii distribution moves to larger values. This is quantified 
by the average $R_s$ radius, whose values, for the different cases, are presented in 
table~\ref{tab:radius}, togheter with the corresponding number of active particles 
(i.e. particles lying in the field of view) and computing time. 
Figure~\ref{fig:panorama} shows the images created by splotch at the different radii.
The image size is kept constant in all the tests, with $N_{pix}$=1000.

\begin{table}
\caption{Average particle radius (left column), number of active particle (center)
and computing time (right column) for the seven tests performed.}
\begin{center}
\begin{tabular}{|l|l|l|}
\hline
$R_s$ (pixels) & Active Particles & Time (sec.) \\
\hline
0.30   & 226894837  & 21.12 \\
\hline
0.62   & 225972201  & 22.50 \\
\hline
0.72   & 212746328  & 21.79 \\
\hline
1.40   & 153647633  & 18.88 \\
\hline
3.90   & 64756141   & 25.22 \\
\hline
8.99   & 13686588   & 28.06 \\
\hline
13.66  & 4222214    & 22.15 \\
\hline
\end{tabular}
\end{center}
\label{tab:radius}
\end{table}


\begin{figure}
\centering
\includegraphics[scale=0.2]{Images/panorama.eps}
\caption{The particle distribution rendered by Splotch at different radii}
\label{fig:panorama}
\end{figure}

The same tests have been carried out also with the original Splotch code, in order 
to compare the performances of the two different implementations. 

In the tests the computing time changes according
to the number of active particle, that decreases approaching the center of the particle 
distribution, and the radius $R_s$, that progressively increases. At values of 
$R_s$ larger than unity, a relevant fraction of the particles has radii comparable to the image size
contributing to a large number of pixels, so increasing the computing time 
with the square of their radius. The two opposite trends tend to compensate, keeping  
the computing time between 19 and 28 seconds.

%\begin{figure}
%\centering
%\includegraphics[scale=0.5]{Images/scala-with-R.eps}
%\caption{Scalability of Splotch with the average radius $R_s$}
%\label{fig:Rs}
%\end{figure}

In figure~\ref{fig:gpucpu}, we present the scalability of the GPU code 
with $R_s$. Since the number of processed particles depends on
the camera position, we adopt the  
computing time per particle as a meaningful performance
indicator. The curve shows a bi-modal behaviour. For small $R_s$,
the computing time is almost constant, since the performance is dominated
by $N_{part}$ term, which is substantially constant in that range of 
radii. For values of $R_s$ larger 
than unity, the dependency from the square of the radius dominates and the computing time 
rises quickly. 

\begin{figure}
\centering
\includegraphics[scale=0.5]{Images/scalaomp.eps}
\caption{GPU and CPU performances at different radii. Black line represents the GPU 
computing time. Blue lines the CPU times from 16 cores (light blue) to 1 core
(dark blue)}
\label{fig:gpucpu}
\end{figure}

Figure~\ref{fig:gpucpu} presents the comparison between the GPU and the CPU computing time
in the seven cases under investigation, using 
1, 2, 4, 8 and 16 cores, exploiting the Splotch's OpenMP \cite{openmpurl} parallel
capability, so that the GPU performance can be directly compared to
that achievable on a multicore, multithreads configuration.
This test allows to get an indication of the actual hardware configuration
needed, in the different cases, to get performances similar to those
obtained with the GPU. The simple comparison with a single core would provide 
just an approximate hint, since parallel codes never scale linearly with
the number of processors. 

The figure shows how, at radii comparable 
or smaller than unity, the GPU's performance is 
comparable to that obtained by 8 cores.
At the same radii, the GPU is from 4 to 5 times faster than the single 
core. As $R_s$ grows, more and more particles are rendered by the CPU, hence 
the performance deteriorates, becoming progressively comparable to that
of 4 and, finally, 2 cores. However, even in
the case with the largest $R_s$, there is some advantage in using the GPU, 
due to the Rasterization stage and since a fraction of the particles 
remain small enough to be rendered by the accelerator.

{\bf GPU overhead}

\begin{figure}
\centering
\includegraphics[scale=0.5]{Images/over.eps}
\caption{Overhead of the various components implemented for the Splotch's
GPU refactoring. Absloute time is shown on the left image. On the right
the ratio between the time spent in each component and the total GPU 
overhead is shown.}
\label{fig:over}
\end{figure}

The same tests presented above, run at different $R_s$, are used also to
analyze the {\it GPU overhead}, that is the time spent by functions specific
to the GPU implementation, that are not present in the original code. Such functions can 
be identified as those to offload data to the GPU (Offload),
to setup the GPU environment (Setup),
to sort and reduce data for tiles preparation and memory displacement optimization (Sort),
to remove non active particles and to pack and copy back particles that have to 
be rendered by the CPU (Select) and, finally,
to combine the Btiles in the final image (Image combine).   

Figure~\ref{fig:over} presents the contribution of the different functions
as absolute (left image) and relative (right image) times. This last 
quantity is defined as the time spent by the specific function devided by
the overall GPU overhead. In the right image, also the fraction of the GPU
overhead to the total computing time is shown (xxx line). It is interesting to 
note that this is almost the 50\% of the computing time for small radii,
while it decreases to less than 30\% at large radii, where the
computing time is dominated by the CPU. 

The Image combine time is negligible, accounting for approximately 0.1\% of 
the GPU overhead. This is due mainly to the efficient design of the 
algorithm, which takes full advantage of the multi-thread architecture 
of the accelerator. The Setup and the Selecton stages contributes 
almost constant to the overhead. Selection, in particular, 
always accounts for almost the 30\% of the time. NON MI TORNA... COSA FA
IN REALTA' "FILTERING"? PERCHE' E' COSI' LUNGO?
Selection, for particle radii larger than unity, becomes the second 
larger contribution to the overhead. For small radii, the Sort
stage accounts for more than 30\% of the overhead. This is essentially due to 
the large number of particles to be sorted according to their tile index. 
The Sort time drops at larger radii, where more and more particles are
inactive.
 
The most relevant contribution to the GPU overhead is represented by the 
Offload. This is constant, since in all tests all particles are copied 
to the device memory. Hence, It becomes relatively more and more relevant at larger
radii, since the overall GPU overhead decreases. In principle, this Offload overhead could
be strongly reduced adopting CUDA streams and overlapping data copy to 
computation. However, this is currently prevented from the adoption of
Thrust libraries, that do not support asynchronous operations. Furthermore,
in some situations, when the CPU workload is larger than that of
te GPU (e.g. at large radii), such solution would be anyway ineffective.  

\section{Conclusions and Next Steps}
\label{sec:conclusions}

The CUDA implementation of Splotch allows to exploit GPUs 
for the visualization of huge data volumes, such those produced both 
by experiments/observations and by computer simulations. Current trends
in HPC envisage the adoption of accelerators to substantially 
increase the performance of the computing systems, maintaining the power
consumption reasonably low. However, accelerators, and in particular 
GPUs, requires a certain effort to be efficiently exploited, 
supporting only peculiar programming models, that requires codes, or part
of them, to be redesigned and reimplemented before being sutable to such devices. 

Splotch proved to be a challenging application to enable to GPUs. 
In fact, the main computational kernel, the Rendering kernel, poses serious difficulties
to the GPU's programming model, that privileges highly data parallel algorithms
where the hundreds of cores of the GPU can work independently from each other.
Any kind of dependency is strongly penalized, reflecting on the final performance.
Splotch, however, violates this basic requirement in the following aspect:
each particle can affect a different number of pixels, depending on its radius.
This leads to frequent concurrent accesses to memory, since
different threads acting on different particles may affect the same pixel,
leading to wrong results. Furthermore, memory usage is particularly 
tricky, since, if not properly managed, 
it can lead to an overwhealming number of random accesses 
with dramatic consequences on the performance.

In order to overcome these difficulties, Splotch's rendering algorithm has been
redesigned with the introduction of specific kernels. These kernels 
introduce an overhead that can be up to the 50\% the overall computing
time. Nevertheless, the final code overperforms the original CPU
version of up to a factor of 4-5, which rises to a factor of 8 when the OpenMP,
multithread parallel version is considered for comparison. Further optimization can
be obtained by the adoption of CUDA streams, overlapping computation 
and data movements. However this is currently prevented by the usage of the 
Thrust libraries, that do not support such kind of operational mode. 
The Splotch GPU code retains also two important features of the original
code: the linear scalability with the number of particles and with the 
image size. 

Although not extraordinary, the attained GPU performance is sufficient 
to run efficiently on the hybrid computing nodes that are more and 
more common in the HPC framework. In order to fully exploit such architectures, 
a further step is necessary, supporting the concurrent usage of 
multiple GPUs, multiple cores and multiple nodes, exploiting respectively the OpenMP and
MPI capabilities of Splotch. This step is expected to pose important 
challenges especially to find the optimal balance in the workload of each of the different 
system components.

\bibliographystyle{plain}
\bibliography{master.bib}	

\end{document}
