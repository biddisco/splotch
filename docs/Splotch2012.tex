\documentclass[11pt]{article}
\usepackage{amssymb,amsmath}
\usepackage{graphicx} 

\title{Splotch on GPUs using the CUDA paradigm}
%\author{M. Rivi, C. Gheller, M.Krokos}
\author{.........}

\begin{document}
\maketitle

\section{Introduction}

The management and analysis of data generated by scientific experiments, 
observations and numerical simulations, currently represent an extraordinary challenge  
both for the research teams who concurred to its production and for 
the data centers, that have to provide technical solutions in terms of 
storage, computing power and software.

The main challenges are due both to the data size, aggregate but also of each 
single dataset, and to its complexity, useful information being hidden in a sea 
of bits. Standard data mining and analysis software often relies on complex
algorithms, that become prohibitively computationally expensive when dealing with 
huge datasets. Visual data exploration and discovery can represent a valuable
support, since it provides a prompt and intuitive insight to
very large-scale data sets to identify regions and/or features of interest within which
to apply time-consuming algorithms. 
Furthermore, this apporoach can be an extremely effective and ready way of discovering 
and understanding correlations,
similarities and data patterns, or to identify anomalus behaviors, that can be
interpreted as a wrong result, avoiding to waist time and effort on the specific dataset or
to save resources in an on-going experiment (e.g. stop a simulation that is producing 
unreliable outputs). Finally, visualization is also an effective means of presenting
scientific results both to experts and to general public.

In order to visualize huge datasets, suitable tools must be available, able to 
exploit High Performance Computing (hereafter HPC) devices, like multicore, multinode
supercomputers, providing suitable resources in terms of computing power, memory size, 
storage capacity and performance and network speed. Currently, not many such tools are avilable.

BIBLIOGRAPHY: visit and paraview, Australian stuff, Tipsy, visivo, stuff from MPI, 3D slicer, Splash

In this paper we focus on {\it Splotch} REF, our previously developed ray-casting
algorithm. Splotch was born for an effective high performance visualization of large-scale 
astrophysical data sets coming from particle-based computer simulations. The software is 
specialized in the high-quality, high-performance rendering of point like data as those 
produced in cosmology by N-Body numerical experiments, which 
represent prime examples of particle-based simulations. We can mention 
the Millennium ``trilogy'', the Horizon run or the DEUS simulation (REF) ETC., which
represent the most advanced and largest simulations in the field. 
This models reproduce the evolution of a meaningful fraction
of the universe by means of hundreds billion fluid elements, represented as particles,
interacting with each other through gravitational
forces. The typical size of a output (``snapshot'') of these simulations is above some hundreds 
of Gigabytes up to tens of Terabytes, and typically stores the
ID, position and velocity together with additional properties, e.g.
local smoothing length, density and velocity dispersion, of each particle.

Splotch however, has been successfully adopted also in other application fields,
like the visualization of real galaxy systems, whose 3D shape is carefully reconstructed
according to observational data. Here, more than the data size, the driver is
the quality and the level of details of the final images,
that have to reproduce the full details of the
spectacular data coming from astronomical observations (REF in preparation). Furthermore,
it has been adopted also for the visualization of meshed based astrophysical simulations, 
although the same high quality cannot be achieved unless extremely high resolution
meshes are provided. 

In the development of Splotch, specific care has been taken of all the performance 
issues. The software is optimized in order to require the minimum possible
memory usage and in order to exploit vector architectures, multi-core processors 
and multi-nodes supercomputers (REF). 

In the last years, 
GPUs, and their computational counterpart, GPGPUs, have 
acquired more and more popularity both in the graphics and in the HPC 
communities, since they can provide extraordinary performances on suitable
classes of algorithms, with speed-up factors of about one order of magintude with respect to 
a standard multicore CPU and with a comparable power consumption.
Therefore, on HPC systems, graphic accelerators are becoming more and more common. 
Many supercomputers are equipped with hundreds of GPUs that can overlap 
their computing power to that of the CPUs, strongly reducing the time-to-solution
of many typical scientifc problems.

In order to exploit this additional computing resources, we have implemented 
a GPU version of Splotch. A full refactoring of the code has been necessary 
to get the good performance out of the GPU's implementation. 
All the details are presented in the rest of the paper. In Section 2, we give a short 
overview of the Splotch main algorithms. In Section 3, we recap the main features
of the GPU architecture and we present the performance model that drove the 
re-design of the code. In Section 4 we summarize the CUDA programming model, that we
adopted for the GPU implementation, and we describe the implemented algorithm. Section 5 shows
the results of the performed tests and benchmarks. Finally, in Section 6, we draw 
the conclusions. 

\section{Splotch Overview}

The splotch code has been designed to visualize point-like datasets coming
from fluid dynamics simulations adopting numerical approaches like N-body (REF)
or SPH (REF). Such approaches are particularly successful in astrophysiscs (e.g. computational
cosmology, galaxy formation, dynamics of star clusters), but are common in 
many other scientific application fields (REF).

The main peculiarities of the Splotch software are represented by the 
high quality of the generated images, obtained adopting a specific ray-casting 
approach, its high performance, due to a strong optimization on HPC architectures and the exploitation of multi-core, multi-node 
systems by means of an effective mix of the OpenMP and MPI parallel programming paradigms, and
the support of large data volumes, through an optimal usage of the memory, with
no data replica or unnecassary allocations, a full 64 bits support and the exploitation
of both shared and distributed memory systems. Furthermore, Splotch's design is kept 
intentionally simple, making its extension to new components and functionalities easy. 
Finally, the software package is completely self-contained, with no dependencies from external 
libraries (apart from those needed for parallelism and HDF5 - REF - for
specific input data file formats). The code is fully C++ and its compilation
usually straightforward, as soon as a suitable makefile script is provided.
No configure procedure is currently supported.

The Splotch algorithm operational scenario consists of a number of stages. 
\begin{itemize}
\item
Data load: data is read from one or more data files. A number of different file
formats are supported, from raw binaries, to HDF5 to more specific Gadget (REF) or Tipsy (REF)
files. The $x$, $y$ and $z$ coordinates are the only compulsory quantities to provide.
These can be cartesian geometric coordinates, but also any other tern 
of variables, adopted to explore a generic three dimensional parameter space.
\item
Data processing: data is normalized, necessary 
transformation (e.g. calculations of logarithms of processed quantities) are performed
and RGB colors (according to read data or to a given
external color table) are associated to each data point. 
\item
Geometry set-up: particle coordinates and other geometric quantities 
(like smoothing lengths, see below) are roto-translated and projected according to the 
camera configuration (camera position, look-at direction and orientation). 
\item
Ray-casting: particle contribution to image pixels are calculated.
\item
Image save: final images are saved using the TGA high quality format.
\end{itemize}
The workflow is shown in Figure , where the parts of the algorithm parallelized
either with MPI or with OpenMP are emphasized. For a detailed description of
the hybrid parallel implementation of Splotch and its performances, we refer to REF.

%\begin{figure}
%\centering
%\includegraphics[scale=0.5]{Images/workflow.jpg}
%\caption{...}
%\label{fig:workflow}
%\end{figure}

The most peculiar part of the Splotch software is represented by its ray-casting algorithm.
This is also its most computationally demanding component.
In order to effectively and efficiently handle point-like distributions, the following 
steps are carried out: 

\begin{itemize}
\item
the particle $p$ with coordinates $\vec r$ transports a quantity $\rho_p(\vec r)$ 
(e.g. the mass density or the temperature at position $\vec r$)
that modulates the contribution of the specific particle to the image
according to the following gaussian distribution: 

\begin{equation}\label{smooth}
\rho_p(\vec r)=\rho_{0,p}\exp(-r^2/\sigma_p^2),
\end{equation}

where $\sigma_p$ is the smoothing length, which represents
a sort of size of each particle.  
In principle, each data point contributes to all the pixels of the final image.
In practice, it is much more handy to have a compact support of the
distribution, that is, the distribution is set to zero at a given
distance $f\cdot\sigma_p$, where $f$ is a proper multiplicative factor.
Therefore rays passing
the particle at a distance larger than $f\cdot\sigma_p$ are
unaffected by $\rho_p$.

\item
Three ``frequencies'' to describe the red, green and blue
components of the radiation, respectively, are used. These are treated independently.

\item
The radiation intensity $\bf{I}$ (treated
as a vector with r, g and b components) along a ray through the simulation
volume is modeled by the well known radiative transfer equation
\begin{equation}\label{rad}
\frac{d\bf{I}(x)}{dx}=(\bf{E}_p-\bf{A}_p\bf{I}(x))\rho_p(x),
\end{equation}
which can be found in standard textbooks REF.
Here, $\bf{E}_p$ and $\bf{A}_p$ describe the strength of radiation emission and absorption
for a given particle for the three rgb-colour components. 
%In general it is recommended to
%set $\bf{E}_p=\bf{A}_p$, which typically produces visually appealing images. This is presently a
%necessary setting for Splotch, in order to reduce the complexity of some aspects of its parallel
%implementation. This constraint will be eliminated in the next releases of the code.
If a scalar quantity is chosen (e.g.\ the particle temperature,
density, velocity dispersion, etc.), the mapping to the three components of $\bf{E}$ and $\bf{A}$ (for red, green and blue)
is typically achieved via a transfer function, realized by a colour look-up table or palette, which can
be provided to the ray-tracer as an external file to allow a maximum of flexibility. If a
vector quantity is chosen (e.g.\ velocity, magnetic field, etc.), the three components of the vectors
can be mapped to the three components of $\bf{E}$ and $\bf{A}$ (for red, green and blue). 
In addition
to the color, the optical depth of each particle can be also modulated proportionally to another
scalar property (e.g.\ density, etc.).
\end{itemize}

Further details on the Splotch rendering algorithm can be found in REF.

\section{The GPU Code}

GPUs can represent an effective tool for Splotch to dramatically 
reduce the time-to-solution and to step toward real-time interaction.
However, the real effectiveness of GPUs on Splotch rastering algorithm has to
be carefully analysed and estimated. Splotch, in fact, poses serious challenges 
to the GPU's programming model, that privileges highly data parallel algorithms 
where the hundreds of cores of the GPU can work independently from each other.
Any kind of dependency is strongly penalized, reflecting on the final performance.
Splotch, however, violates this basic requirement in the following aspect:
each particle can affect a different number of pixels, depending both on its intrinsic
smoothing radius and on the camera position (the same particle 
can affect none or all the pixels of an image, depending on the point of view). 
Hence, an optimal load balancing can be hard to achieve 
and, even worse, it leads to continuous concurrent accesses to memory, since
different threads acting on different particles may affect the same pixel, trying
to update its value, with the loss of one or more contributions.
The quality of the resulting image is therefore degraded and its correctness may be
compromised. 

The potential dependency of any pixel of the image from any data point,
makes the distribution of work between GPU's cores challenging to implement
and even harder to tune in terms both of performances and of memory requirements.
In the section XXX, we propose a simple performance model that has
driven the implementation of the GPU enabled Splotch kernels.

\subsection{The GPU architecture}


% RIVEDERE QUESTO PARAGRAFO
The design of GPUs is optimized for the execution of large number of threads dedicated to floating-points calculations: 
many-cores with a minimized control logic unit and large memory bandwidth in order to manage leightweight threads, maximize execution throughput and overcome long-latency memory accesses. 

For example, the Fermi architecture (see Figure...) is made of 512 cores grouped in 16 multiprocessors of 32 cores each. Memories are organized according a hierarchy reflecting the organization of the computing elements. The board contains a global memory (4GB) with a 1.5x bandwidth (GDDR5) and long access latencies (hundreds of clock cycles), a read-only constant memory (64kB) and a read-only texture memory. To reduce long access latencies to the global memory, a unified L2 cache (768kB) has been introduced. On the chips there are some registers for each hardware core and a shared memory (64kB) for each multiprocessor (16kB of it are used as L1 cache private to thread). They can be accessed at very high speed in a highly parallel manner and are the most dangerous occupancy limiting factor.
 
\subsection{The CUDA programming model} 

For the implementation of the GPU code we have adopted the CUDA programming model (REF). This is currently the standard ``de facto'' for GPU programming.
Its main drawback is represented by the limited portability, being an NVIDIA product. However,
this ensures that the CUDA programming model closely maps and supports the underlying
hardware (NVIDIA cards), leading to an optimal tuning of the performance.
The CUDA standard offers access to highly parallelized modern GPU architectures via a simplified C/C++ or Fortran language interface. It is designed to support joint CPU/GPU execution of an application, where serial sections are performed by the CPU (host), while those which exhibit rich amount of data parallelism, are performed by the GPU (device) as CUDA kernels. The CPU and the GPU have separate memory spaces so data must be transferred from each other via PCIe bus. CUDA kernels launch is asynchronous, so that the host can
 perform the following instructions of the code while the device is computing. If one of them requires the completion 
of the kernel execution, then it is necessary to call the \textit{cudaThreadSyncrhonize}() function before executing it.
A kernel is written for a single thread and instantiated as a grid of many lightweight parallel threads, organized into blocks of the same size. A thread is an indipendent element of work and maps to a hardware core. A block is a 1D, 2D or 3D set of concurrently executing threads that can cooperate among themselves through barrier synchronization and "fast" shared memory. 
This is possible because threads belonging to the same block are executed on the same multiprocessor (SM). On the other hand
 synchronization is not possible between blocks of a grid. In fact, the limited amount of memory limits the number of 
blocks that can simultaneously reside in the same SM. Moreover when one block stalls the runtime system switch to 
a different one, hence there is no guaranteed order of execution.

Once a block is assigned to a SM, it is partitioned into 32-thread units called warps. They are scheduling units in SM:
all threads in a same warp execute the same instruction (Single-Instruction, Multiple-Thread). Hence, programmers should minimize the number of execution branches inside the same warp. It is convenient to assign a large number of warps to each SM (i.e. high occupancy), because the long waiting time of some warp instructions is hidden by executing instructions from other warps  and therefore the selection of ready warps for execution does not introduce any idle time into the execution timeline (zero-overhead thread scheduling). Further details on CUDA can be found in (REF).

\subsection{The Performance Model}

% probabilmente questa e' una ripetizione
Splotch's main computational kernels are the {\it Normalization}, re-calculation
of the different quantities in proper units, {\it Geometry}, roto-translation 
of the reference frame, {\it Coloring}, assignment of the RGB colors associated to each 
particle,  {\it Rendering},
rasterization and ray casting, 
and {\it Image Creation}, composition and save of 
the final image. The initial data load phase, though often demanding, will not be considered
since it is not subject to the GPU implementation.

The main parameters of the model are $N_{part}$, the number of particles
to be processed, and $N_{pix}$, the number of pixels of the final image
(in general we can consider square images with $N_{pix}=N_{pix,x}^2$). For 
the Splotch's rendering kernel a third parameter must be introduced, 
that is the average smoothing length of the particles,
$R_s$. Its value depends both on the intrinsic "size" of the particle
and on the position of the point of view (particle size increases getting closer to 
the point of view).

%The kernels scale with these parameters as:
%\begin{align}\label{scaling}
%& N_{Norm} \propto N_{part},\\
%& N_{Geom} \propto N_{part},\\
%& N_{Color} \propto N_{part},\\
%%& N_{Sort} \propto N_{part}{\rm log}(N_{part}),\\
%& N_{Render} \propto N_{part} R_s^2,\\
%& N_{Image} \propto N_{pix,x}^2,
%\end{align}
%where $N_{kernel}$ is the number of operations for a given kernel.

%The Normalization, Geometry and Coloring kernels are  
%perfectly data parallel, each particle being processed independently from the others.
%Thus they are expected to fit the GPU programming model. 
%The Rendering kernel is, in general, the most computationally demanding. 
%The computational time depends in a predictable way with the number of particles and the number 
%of pixels. The most challenging dependecy, however, is from the smoothing length. 
%This dependency leads to strong changes in the computational effort and makes
%the work-load hard to balance, when this is distributed among many-processors
%systems.

In order to model the Splotch algorthm's performances, we split the different timings
required to perform the main computational steps. 
The Splotch's performances can be quantified as time spent on the CPU, time spent on the GPU
and time spent to move data among different memories:
\begin{equation}\label{Ts}
T_{TOT} = T_{cpu} + T_{gpu} + T_{pci} + T_{Mgpu},
\end{equation}
where $T_{TOT}$ is the total time, 
$T_{cpu}$ is the time spent on the CPU, $T_{gpu}$ is the time
spent on the GPU for computation, $T_{pci}$ is the time needed to move data from
the CPU to GPU and back through the PCI Express bus and $T_{Mgpu}$ is the time 
spent in moving data among the global and the shared memory on the GPU.

Time spent in transferring data
between CPU and GPU, $T_{pci}$, is typically the main 
bottleneck in GPU usage. Therefore the amount of data transferred from host
to device and back has to be minimized. 
All particle data has to be offloaded to the GPU. If $S_{part}$ is the
size of a single particle (in the current implementation $S_{part}=35$ bytes),
$N_{part} S_{part}$ is the total amount of particle data moved to the device.
The final processed image (the result) is the only data that has to be copied back to
the CPU. Thus, in principle, only $3 N_{pix,x}^2$ bytes (R, G, B values) have to be transferred at
the end of the rendering phase. However, further data transfers may be necessary
during the computation. Such data movements should be hidden
by overlapping them with calculation, exploiting CUDA asyncronous operations.

Particles' data offload can be accomplished either in a single or in a few operations, 
depending on the data size (if it fits the available GPU memory) and/or
specific features of the algorithm (in some cases it may be convenient to
split data into chunks, copying on the GPU one chunck after the other).
In general we can estimate the data transfer time as:
\begin{equation}\label{pci}
T_{pci} =  N_{chunks} \tau_{pci} + {N_{part} S_{part} + 3 N_{pix,x}^2 \over 
\mu_{pci}},
\end{equation}
where $\tau_{pci}$ is the transfer time latency (in seconds) and $\mu_{pci}$ is the
bus bandwidth (in bytes per second), $N_{chunks}$ is the number 
of copy stages. 

The time spent in processing the particles on the GPU can be estimated as:
\begin{equation}
T_{gpu} = N_{op}/\nu_{GPU},
\end{equation}
where $\nu_{GPU}$ is the GPU flops/sec rate and
\begin{equation}\label{ops}
N_{op} = N_{part}(\alpha + \gamma R_s^2) + f_{GPU},
\end{equation}
with $\alpha$ and $\gamma$ estimating the number of operation of 
the combined Normalization, Geometry and Coloring kernels,  
and of the Rendering kernel respectively. The function 
$f_{GPU}$ takes into account any GPU specific part of the algorithm. 

Global memory accesses can be estimated 
as the number of loads from the global to the shared memories of the available 
streaming multiprocessors plus the number of stores from the shared to the global memory: 
\begin{equation}
N_{Mgpu} = (N_{load,p} + N_{store,p}) N_{part} + N_{store,pix} N_{pix,x}^2,
\end{equation}
where $N_{load,p}$ and $N_{store,p}$ are the number of load and store of the 
particles, while $N_{store,pix}$ is the number of stores of the image (no loads 
are expected, since the image is created on the GPU). 
The time for memory access is:
\begin{equation}\label{tmgpu}
T_{Mgpu} = {(N_{load,p} + N_{store,p}) N_{part} S_{part} 
+ 3 N_{store,pix} N_{pix}^2\over \mu_{gpu}}
+ g_{GPU},
\end{equation}
where $\mu_{gpu}$ is the global memory bandwidth and $g_{GPU}$ the time 
spent on memory accesses by the GPU specific
functions, corresponding to the $f_{GPU}$ term in equation \eqref{ops}.
The terms $f_{GPU}$ and $g_{GPU}$ will be discussed in details in section XXX.

The final contribution to equation \eqref{Ts} is $T_{cpu}$. In principle this should be negligible, 
the large part of the work being performed by the GPU. Actually, the CPU
could contribute in processing a proper fraction of the particles. Using
asyncronous operations, however, this work can be overlapped 
to the GPU work, as described in section XXX.

\subsection{Performance Model analysis}

Three classes of parameters characterize the performans model.
the number of particles $N_{part}$,
the image size $N_{pix,x}^2$ and the characteristic particle size $R_s$ belong to
the first class. They 
are \``model" parameters, related to the specific dataset or to the image quality.
A second class of parameters is related to the algorithm and the way 
it has to be designed in order to exploit the architectural characteristics
of the GPU. The identified parameters are $N_{chunks}$, $N_{load,p}$, 
$N_{store,p}$,  and $N_{store,pix}$. Furthermore, $f_{GPU}$ and 
$g_{GPU}$ have to be properly described and their impact on the performances
estimated (see section XXX). The final class of parameters 
is specific to the GPU architecture. In our analysis we have set them to fiducial
values related to the NVidia M2090 Fermi (REF) GPU, that was used for our
tests and benchmarks. More specifically: 
$\mu_{gpu} = 130$ GB/sec, $\tau_{pci} \sim 10^3$ nsec and
$\mu_{pci} \sim 6$ GB/sec.
A further set of parameters, is related to the CUDA progeamming model. The optimization
of parameters like the number of blocks, the number of threads per block etc., will 
be described in details in section XXX.

Equations $\eqref{pci}$ and $\eqref{tmgpu}$ show that performances
depends linearly from both the number of particles and the number of pixels. 
In the case of large datasets we have that
$N_{part} >> N_{pix,x}^2$. For instance, the test case adopted 
in section XXX uses $N_{part} \sim 10^8$
and $N_{pix,x}^2 \sim 10^6$. 
In that case, the contribution of $N_{pix,x}^2$ 
to $T_{pci}$ is negligible. 
The term $T_{Mgpu}$ appears to have a comparable behaviour. However, 
this holds only if all the particles falls in the field of view (so in
the rendered scene),  unless the 
parameter $N_{store,pix}$ is of the order of 100 or larger. Hence, the algorithm has to
be designed such that the number of copies of the image's pixels to the 
global memory is $O(1)$. At that point, all the dependencies on the images size can
be neglected. If instead the point of view is placed such that part of the particles 
falls outside the field of view (e.g. inside the particle distribution), these 
particles do not contribute to the calculation and the two terms of equation 
\eqref{tmgpu} can become comparable.

Equation \eqref{ops} shows that $N_{op}$ has a critical dependency from 
$N_{part} R_s^2$. The dependency from the product of these two parameters, makes 
this contribution the most important from a computational point of view and the most
tricky to model and to tune. This is due especially to the unpredictability of
both terms, that depend on the position of the point of view. In particular, 
the $R_s$ parameter can lead to a strong increase of the computing time, since 
it can be large, involving many pixels and huge unbalances in the processing 
time of each single particle. This
represents a big issue in the case of parallel processing, as in a multi-core or
many-core system, where a good workload balance is crucial to achieve good performances.

Data offload to the GPU is one of the crucial steps in the algorithm. 
Equation \eqref{pci} shows that we have a minimum amount of data 
to be moved to the GPU: the particles information (coordinates, fields, smoothing length). This leads to an 
unavoidable overhead, that, however, can be masked by overlap with computation using
CUDA asynchronous data transfer. In order to implement this solution data have to be
splitted in chunks and passed to the GPU one after the other, in a loop. Whilst chuncks 
are transferred, computation on previous chuncks, already stored on the GPU
memory, can be carried out. Splitting data in sub-chunks can also be necessary
in order to fit data in the GPU global memory, since the whole dataset could exceed
that available on the device. According to equation \eqref{pci}, this does not 
cause any meaningful overhead if the copy latency is kept small, that is:
\begin{equation}
N_{chunks} < {N_{part} S_{part}\over \tau_{pci}\mu_{pci}}.
\end{equation}
For instance, if $N_{part}$ is $O(10^8)$, $N_{chunks}$ can be up to $10^5$ before
contributing to the overhead.

Equation \eqref{tmgpu} show that the performance related to global memory access
depends on the two parameters $N_{load,p}$ and $N_{store,p}$, that quantify the 
total amount of data transferred from the global to the shared memories and back,
whose performance depends on the memory bandwidth.
The general rule is to keep these parameters
as small as possible. 
The optimal solution would be to have one thread 
processing one particle. In this case, we would have $N_{load,p} = 1$ and $N_{store,p} = 0$.
This solution, though ideal, is not practical. The main reason is the following: 
if a thread processes a particle, it updates various pixels leading to frequent 
race conditions with other threads trying to concurrently update the same pixels.
This is very likely to happen when thousands of threads works at the same time, 
leading to loss of information and corrupted images. This problem has to be treated 
in a specific way, designing a proper algorithmic solution. This will be 
described in details in section XXX. However, part 
of the algorithm is data parallel and can adopt the \``one thread per particle" solution.
This corresponds to the Coloring and Geometry kernels, in which each particle 
is processed independently from the others. Merging this two components in a single
kernel, we have $N_{load,p} = 1$ and, since processed particles
have to be copied back to the global memory to be rendered, $N_{store,p} = 1$. 

\section{The CUDA implementation}

As highlighted in section XXX, porting the Splotch's rendering 
algorithm on the GPU requires a specific re-design and refactoring.
Two main issues must be faced:
\begin{itemize}
\item 
race conditions writing different contributions to the same pixel,
\item
workload unbalace between different processing units. 
\end{itemize}
The rendering kernels have to be designed according to the prescriptions of 
the performance model and in order to circumvent the two issues above.

\subsection{Algorithm design}

% SOME LINK TO THE PERFORMANCE MODEL NEEDED HERE


As soon as data is loaded in the global memory of the GPU, each particle is processed by a single CUDA thread. The processing involves transformation into screen coordinates (geometry) and colours assignment (coloring).
Rendering requires instead a much more carefully implementation, mainly due to 
the granularity during rendering can vary considerably depending upon the number of screen pixels influenced by individual particles, the workload of this kernel had instead to be designed more carefully.
In fact, as a worst case scenario, consider two particles influencing all screen pixels and a single screen pixel
respectively. Assuming they are handled by two threads of the same warp, they are scheduled to execute simultaneously compromising significantly overall execution times.

%SAY BETTER SENTENCES ABOVE
Particles are first classified in 3 classes according to their size:
\begin{itemize}
\item 
C1: $r > r_0$
\item
C2: $1 < r < r_0$
\item
C3: $r < 1$
\end{itemize}
where $r$ is the particle smoothing length in pixels, which depends both on theintrinsic properties of the particle (the ``physics'' of the problem) and on the camera position, and $r_0$ is a fixed threshold. Each class of particles is rendered in the most convenient and efficient way by making CPU and GPU work in parallel. Because of the limited device resources and the possible large variation of the workload among big particles, we choosed to assign C1 particles to the host. Remaining particles can influence only a small region, thus they are suitable to be rendered by the device: C2 particles can be calculated on the GPU using the tiling scheme described below, while C3 particles, which affect only one pixel, can be computed in a simplified way. Finally, partial images generated by C1, C2 and C3 particles are added to get the final result. 

The particle is classified on the GPU in the in the process kernel, where each particle is labelled with an integer number $n$ as follows:
\begin{itemize}
\item 
$n = -2$, if it is not active, i.e. out of the view;
\item
$n = -1$, if it belongs to C1 class; 
\item
$0 < n < ntiles$, if it belongs to C2 class and its centre falls in the $n$-th tile of the image;  
\item
$n = ntiles$, if it belongs to C3 class, i.e. is point-like.
\end{itemize}
By sorting the array of particles by key $n$, we are able to group them according to their type in order to copy back to the host C1-particles, remove from the device inactive and C1 ones, count and distribute C3 and C2 particles belonging to the same tile for rendering. 

Sorting operation has a crucial role in the CUDA version of Splotch because it allows to manage particles on the device and it is necessary for executing in an efficient way further operations like reduction and prefix sum by key. In fact, memory accesses to the elements involved in a CUDA operation are optimized when all of them are consecutives. Since the sort is intrinsically an expensive operation, we had to find a very good CUDA implementation of it in order to reduce its overhead. 
The Thrust library (REF) provides one, this is a C++ template library for CUDA which mimics the Standard Template Library (STL) and provides optimized functions to manage very large arrays of data. In particular Thrust functions implement a highly-optimized Radix Sort algorithm (REF) for sorting primitive types (e.g., char, int, float, and double) with the standard less comparison operator and apply dynamic optimizations to further improve its performance.

The \textit{tiling scheme} for rendering C2 particles on the device consist in assigning particles related to the same image tile to a block of CUDA threads and exploit the shared memory and thread synchronization within the block to store and compose the image tile. The tile size is defined in order to be able to store it with a boundary of $r_0$ pixels around (this ensure that each particle will be entirely contained in the tile+boundary). We will refer to it as a \textit{Btile}). Particles are accessed one a time by thread 0 of each block and stored in the shared memory so that all threads of the same block can see their data, then each pixel of the current particle is rendered by a different thread of the block (the pixel number processed by each thread may change as the particle varies, but it remains in the Btile). Since all blocks must have the same size, we set it as the maximum C2 particle size, i.e. $(2r_0)^2$. 
This solution avoids race conditions to compose the image tile because each thread of the same block accesses different pixels and the workload of each thread (except thread 0) is the same even if particles may have different size.

When all particles of the block are rendered, the contribution of the Btile is added to the image in the global memory. First, each thread adds the value of $k = tile\_size/block\_size$ different pixels of the inner part of the Btile (there are no race conditions to access the global memory: each block accesses different tiles and each thread accesses different pixels of the tile). The boundary must be added in a separate step. Concurrences among the boundary of a tile and the adjacent ones can occur only between:
\begin{itemize}
\item
either a boundary side and the inner part of another Btile (2 concurrences), 
\item
or a corner and the inner part of another Btile, the horizontal boundary side of another Btile, the vertical boundary side of another Btile (4 concurrences). 
\end{itemize}
As a solution we implemented the following 3 steps (see Figure ...):  
first add a and b sides (no race conditions between boundary sides of tiles nearby),
then add b and c sides (no race conditions between boundary sides of tiles nearby),
finally add corners (no race conditions between corners). Notice that the number of pixels of the 4 boundary corners of a tile are equal to the size of a particle, i.e. the size of a block. Since CUDA blocks are order independent, we need to add these contributions to 3 different copies of the image to avoid race conditions. Then, at the end of the execution of the render kernel it is possibile to add these image copies to the final one.

Another way to compose the image would have been to store all pixel contributions in a fragment buffer and reduce it by key (pixel index). But this solution is worst than ours because it requires the sorting of fragments, instead of particles, which are a much higher number of elements and therefore more time-consuming.

Rendering of C3 particles is much more simple because they are made of one single pixel and therefore their intensity is already defined (we do not need to distribute it among several pixels as for the other types of particles). Thus the only computation required is the position of the pixel in the global image. This can be efficiently done by a CUDA thread for each particle. Since different threads can affect the same pixel, they cannot directly add their contribution (fragments) to the image. For this reason, it is obtained in a second step by adding all fragments contributing to the same pixel. In order to perform this operation on the device and transfer to the host only the final image, we had to allocate in the device memory one buffer to keep trace of the corresponding linear index of the pixel affected by the fragment. Then, the image is produced by reducing by key (pixel index) the fragments. As mentioned before, this computation on the GPU requires a sort of the fragments by key first. Both these operations are provided by the Thrust library.

Because of the size of the GPU global memory, particles data couldn't be stored in a single step. Therefore this algoritm could be performed serveral times, each one producing a partial image that will be composed with the others in order to get the final one.

The pseudo-code below summarizes the overall workflow of this CUDA parallelization approach.

\small
\begin{verbatim}
Algorithm
begin
  setup colormap
  nP = number of particles of each chunk
  tile_sidex = tile sidex size 
  tile_sidey = tile sidey size
  nxtiles = resx/tile_sidex
  nytiles = resy/tile_sidey
  width = width of the tile boundary
  nC3 = number of point-like particles
  allocate device particle array d_part[nP]
  allocate device index tile array d_active[nP]
  allocate device index pixel array for C3 Particles d_index[nP]
  allocate tiles array d_tiles[nxtiles*nytiles]   
  allocate device images of resolution (resx,resy)
  
  initialize Image to 0
  for each chunk of particles: 
     initialize d_image to 0
     copy chunk from host to device  
     num_blocks = (nP + block_size - 1)/block_size
     k_process<<<num_blocks,block_size>>>(...)
     classify particles -> d_host_part, nHostPart, nC3 
     if (nHostPart > 0) then
        copy d_host_part from the device to the host 
     endif
 
     remove host and inactive particles from d_part (nP-> newnP) 

     count and assign particles to the corresponding tiles:
      thrust::sort_by_key(d_active, d_active + newnP, d_part)
      initialize d_tiles to 0
      new_end = thrust::reduce_by_key(d_active, d_active + newnP, 
                           thrust::make_constant_iterator(1), d_active, d_tiles)
      nC3 = 0
      if (d_active[new_ntiles-1] == nxtiles*nytiles) then
         nC3 = d_tiles[new_ntiles-1]
      endif

      thrust::inclusive_scan(d_tiles, d_tiles + new_ntiles, d_tiles)

     if (nC3 > 0) then
         num_blocks = (nC3 + block_size - 1)/block_size
         k_renderC3<<<num_blocks,block_size>>>(nC3, d_part+nP-nC3, d_index)
         thrust::sort_by_key(d_index, d_index+nC3, d_part+newnP-nC3)
         new_nC3 = thrust::reduce_by_key(d_index, d_index+nC3, d_part+newnP-nC3, 
                   d_index, d_part+newnP-nC3, thrust::equal_to<int>, sum_op()) 
         new_ntiles = new_ntiles - 1 
     endif

     k_renderC2<<<new_ntiles,4*width*width>>>(newnP, d_part, d_tiles, ...)
     host_rendering(host_part,nHostPart, Image) 
 
     cudaThreadSynchronize()
     k_add_images<<<num_blocks,block_size>>>(...);
     copy device_image from the device to the host
     Image = Image + device_image
  endfor
end
\end{verbatim}
\normalsize
The CUDA kernel algorithm for C2-particles rendering is the following. 
\small
\begin{verbatim}
k_renderC2
begin
   size = (2width+tile_sidex)*(2width+tile_sidey)
   allocate on the shared memory tile+boundary Btile[size]
   tile = d_active[blockIdx.x] 
   width = r0

   if (threadIdx.x == 0) then
      end = d_tiles[blockIdx.x]
      if (blockIdx.x == 0) then 
         local_chunk_length = end
      else 
         local_chunk_length = end - d_tiles[blockIdx.x-1]
      endif
  endif

  xo = (tile/nytiles)*tile_sidex - width
  yo = (tile%nytiles)*tile_sidey - width
  initialize Btile to 0

  for i=0, ..., local_chunk_length:
     if (threadIdx.x == 0) then
       p = particle to process = d_part[m+i]
       posx = centre x-coordinate of p
       posy = centre y-coordinate of p
       r = radius of p
       color = color of p
       recompute minx,maxx,miny,maxy
       reg = (maxx-minx)*(maxy-miny)
     endif
     __syncthreads()

     render a pixel of particle i:
     if (np < reg) then
       x = threadIdx.x/(maxy-miny) + minx
       y = threadIdx.x%(maxy-miny) + miny
       lp = (x-xo)*(tile_sidey+2*width) + y-yo
       if ( dist((x,y),(posx,posy)) < r*r ) then
         Btile[lp] = color*exp(-1/(sigma*sigma*r*r)*dsq)
       else 
         Btile[lp] = 0
       endif
     endif
  endfor
  __syncthreads()

  add inner part of Btile to d_image 
  __syncthreads()
  add Btile boundary columns (a and b) to d_imageBcol
  __syncthreads()
  add Btile boundary rows (c and d) to d_imageBrow
  __syncthreads()
  add Btile boundary corners to d_imageBcor
end
\end{verbatim}
\normalsize   

The main drawbacks of this implementation is the overhead due to sorting and reduction operations, even if its cost has been kept as low as possible, and the size of the shared memory which limits the number of resident blocks per multiprocessor during the execution of the kernel, thus reducing the theoretical occupancy. Another issue that impacts on the performance is due to the unbalanced work load of each block, in fact the number of particles falling in each tile can vary in a significant way from one tile to another.  
% THIS (ABOVE) HAS TO BE REFORMULATED REFERRING TO MEMORY USAGE, PERFORMANCES AND THE PERF MODEL...

\section{Tests and Results}

We have analyzed the performances of Splotch's GPU implementation on a number of cases, designed in 
order to stress the different features of the algorithm, comparing the results 
obtained using the GPU to those achievable with the original code on the CPU. 
The tests have been repeated using various models of processors and graphic cards, 
in order to verify the behaviour of the code on different accelerated architectures.

Table \ref{table:tests} summarizes the main characteristics of the datasets adopted in the 
tests. The 100M dataset (result of a cosmological N-body simulation) 
is composed by 100 million particles, distributed in 
a cubic box. Each particle is characterized by 3D cartesian coordinates,
mass density at the particle position (that will serve as the color),
and smoothing length, specific to each particle. This test is particularly 
memory demanding (the dataset, alone, requires 2 GB 
of resident memory). The data size is not an issue for the GPU, since data are splitted 
in smaller chunks and loaded iteratively on the accelerator, but it can be a limiting factor
for the CPU, since only CPUs with memories of 4 GBs or larger can run the test. 
In order to have a test suitable to any memory size, 
a smaller version of this dataset, 10M, was generated 
randomly selecting the 10\% of the 100M particles. 
In the 100M and 10M datasets the particles are distributed 
statistically homogeneously (see Figure XXX), hence approximately the same 
number of particles ``falls" in any sub-set of the image (unless
the point of view is not too far away from the data box, leading to a black, empty 
frame around the box). The main goal of these tests is to measure the performances
when no work balancing problems arises, so evaluating the GPU when computing 
power and memory accesses are exploited in the best possible way. 

The M83 dataset is characterized by a particle distribution 
heterogeneous at all scales. For this dataset, each particle is characterized by its 
cartesian coordinates, a smoothing length and the three RGB components of the color. 
In this case, particles are not colored providing a look-up table,
but have an intrinsic color. The dataset is made of XXX million 
particles, with a  disomogeneous distribution which allows to stress 
and analyze the balancing of the workload on the computing system.

\begin{table}
\caption{Datasets adopted for the tests}
\centering 
\begin{tabular}{l c c c} 
\hline\hline 
ID & N. particles & N. fields & Memory (GB) \\ [0.5ex] 
%heading
\hline % inserts single horizontal line
100M   & 100000000 & 5 & 1.863 \\ 
10M    & 10000000  & 5 & 0.186 \\
M83    & XXX & 7 & XXX \\
\hline 
\end{tabular}
\label{table:tests}
\end{table}

The main tests' parameters are the data size (number of particles),
the image size (number of pixels) and the distribution of the number of pixels 
per particle, which is influenced by the position of the point of view.
This is a crucial parameter, affecting the effective smoothing lenght of the particles,
defined as the number of pixels which a particle is distributed on, that 
depends both on the intrinsic smoothing length, set at initialization, and 
on the distance of the particle from the point of view. Other important parameters
are related to the GPU architecture, as the number of cores, the memory size and bandwidth, and the algorithm, as the threshold particle size $r_0$ (uqual to the boundary width of the tile) and the tile size.

Our tests have been performed on a number of different architectures, 
whose characteristics are summarized in Table \ref{table:gpus}. 

\begin{table}
\caption{NVIDIA GPU architectures used for the tests}
\centering
\begin{tabular}{l c c c c c}
\hline\hline
Model   & capability & n. cores & Mem. size & Mem. bandwidth & Mem. type  \\ [0.5ex]
        &      &    & (GB)      & (GB/s)  & \\
%heading
\hline % inserts single horizontal line
GTX 285 &  & 240   & 2.0 & 159   & GDDR3 \\
GTX 480 &  & 480   & 1.5 & 177.4 & GDDR5 \\
M2070   & 2.0 & 448   & 6.0 & 150   & GDDR5 \\
GK104   & 3.0 & 1536  & 4.0 & 160   & GDDR5 \\
\hline
\end{tabular}
\label{table:gpus}
\end{table}
 


\section{Conclusions and Next Steps}

\end{document}


The contribution to each pixel is calculated according to \eqref{rad} and this
can be performed by a single thread in a block (following the CUDA programming model REF).
Once a contribution is ready, it has to be added to the corresponding pixel in the 
image, but at this point the GPU implementation finds a pitfall. Concurrent 
updates of the same pixel by different thread lead to incorrect results.
This can occur frequently, leading to a completely wrong result. A possible solution
to this problem, without using slow atomic updates (that would lead to
a dramatic performance drop), is to define 
an auxiliary buffer, namely the {\it fragment buffer}, where 
the different contributions are stored separately together with the corresponding 
target pixel, and to proceed to image calculation in a dedicated final step.
The fragment buffer is highly memory demanding, and this final step can be
computational expensive. Specific care must be devoted to it. The details are presented in 
section REF.  




the largest possible number of particles has to be transferred
to the GPU in a single operation. This can be calculated as follows:
\begin{equation}\label{MGPU}
M_{GPU} = N_T S_{part} + N_T (S_{FB} N_{pix}+16) + S_{image},
\end{equation}
where $N_T$ is the maximum number of particles that can be be loaded on the GPU,
$S_{FB}$ are the sizes of a particle and of a fragment buffer element in bytes,
$N_{pix}$ is the average number of pixels affected by a particle, $S_{image}$
is the size of the final image (usually negligible). Finally, $M_{GPU}$
is the GPU memory size (in bytes). Therefore, if in \eqref{MGPU} we drop the image size,
we have:
\begin{equation}
N_T = {M_{GPU} \over S_{part}+S_{FB} N_{pix}+16}
\end{equation}
where the 16 term accounts for the two pointers needed for each particle to
manage the frame buffer.


Splotch is a command line application, all the parameters being passed through
a textual parameter file.
This approach completely lacks of any tool for an interactive usage of Splotch,
like a Graphical User Interface (GUI), that would make the description of the scene
and the tuning of the basic parameters much easier, faster and flexible. Currently
this is accomplished by a \``trial and error\'' approach, that is relatively
time consuming and unconvenient. The GUI was not provided in order to keep
Splotch simple and portable, avoiding complex dependencies with external packages and
long and often annoying build procedures. Furthermore, supercomputers computing
engines do not in general provide any kind of support for the graphics, so, the
generation of single images is the only approach that can be used on
such systems. Finally, when used on a personal workstation (PC, laptop etc.),
a real-time approach is prevented by the large size of the data Splotch is
designed to work with (that can fills the entire memory) and the complexity
of the algorithm (we will give more details below), that make any kind of
on-the-fly interaction slow if not impossible.

